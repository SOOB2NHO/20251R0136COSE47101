import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error
from sklearn.model_selection import ParameterGrid
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
from transformers import ElectraTokenizer, ElectraModel
import torch
import torch.nn as nn
import torch.nn.functional as F

# ========================================
# 0. Setâ€Transformer ê´€ë ¨ í´ë˜ìŠ¤ ì •ì˜
# ========================================
class FeatureEmbedding(nn.Module):
    """
    ê° í”¼ì²˜ ì´ë¦„ê³¼ ê°’ì„ ë°›ì•„ì„œ:
      1. name_embedding: feature_name â†’ â„^{d_name}
      2. value_projection: feature_value â†’ â„^{d_val}
    ë‘ ë²¡í„°ë¥¼ concatí•˜ì—¬ â€œì»¬ëŸ¼ í† í°â€ e_feature âˆˆ â„^D ìƒì„±
    """
    def __init__(self, all_feature_names: list, d_name=16, d_val=16):
        super().__init__()
        self.d_name = d_name
        self.d_val = d_val
        self.D = d_name + d_val

        # 1) name embedding table: ê³ ì •ëœ â€œí•©ì§‘í•©â€ feature_namesë¥¼ ì‚¬ì „(dictionary)ìœ¼ë¡œ ê´€ë¦¬
        self.name2idx = {name: idx for idx, name in enumerate(all_feature_names)}
        self.name_embed = nn.Embedding(len(all_feature_names), d_name)

        # 2) value projection: 1-d â†’ d_val-dim ë²¡í„°
        self.value_proj = nn.Linear(1, d_val)
        nn.init.xavier_uniform_(self.value_proj.weight)

    def forward(self, feature_names: list, feature_values: torch.Tensor):
        """
        feature_names: Python list of strings, ê¸¸ì´ = n_features
        feature_values: Tensor, shape = (batch_size, n_features)
                        (ì—¬ê¸°ì„œëŠ” batch_size=1ì¼ ìˆ˜ë„ ìˆìŒ)
        returns: Tensor, shape = (batch_size, n_features, D)
        """
        device = feature_values.device
        n_features = len(feature_names)

        # 1) name ì„ë² ë”©
        name_indices = torch.tensor(
            [self.name2idx[n] for n in feature_names],
            dtype=torch.long,
            device=device
        )
        # name_embed: shape = (n_features, d_name)
        name_emb = self.name_embed(name_indices)  # (n_features, d_name)

        # 2) value projection: feature_values: (batch_size, n_features) â†’ unsqueeze(-1) â†’ (batch_size, n_features, 1)
        val = feature_values.unsqueeze(-1).float()  # (batch_size, n_features, 1)
        val_proj = self.value_proj(val)             # (batch_size, n_features, d_val)

        # 3) name_embì„ batch ì°¨ì›ì— ë§ì¶° í™•ì¥
        name_emb_batch = name_emb.unsqueeze(0).expand(
            feature_values.size(0), -1, -1
        )  # (batch_size, n_features, d_name)

        # 4) concat â†’ token_embeddings: (batch_size, n_features, D)
        token_emb = torch.cat([name_emb_batch, val_proj], dim=-1)  # (batch_size, n_features, d_name+d_val)

        return token_emb  # (batch_size, n_features, D)


class SetTransformerEncoder(nn.Module):
    """
    ê°„ë‹¨í•œ Selfâ€Attention ê¸°ë°˜ ëª¨ë“ˆ:
      - ì…ë ¥: (batch_size, n_features, D)
      - MultiheadAttention â†’ residual + layerâ€norm â†’ feedâ€forward â†’ residual + layerâ€norm
      - ì¶œë ¥: (batch_size, D) ë¡œ í‰ê·  í’€ë§
    """
    def __init__(self, D, n_heads=4, dim_ff=64, dropout=0.1):
        super().__init__()
        self.D = D
        self.n_heads = n_heads

        self.attn = nn.MultiheadAttention(embed_dim=D, num_heads=n_heads,
                                          batch_first=True, dropout=dropout)
        self.ln1 = nn.LayerNorm(D)
        self.ff = nn.Sequential(
            nn.Linear(D, dim_ff),
            nn.ReLU(),
            nn.Linear(dim_ff, D),
            nn.Dropout(dropout)
        )
        self.ln2 = nn.LayerNorm(D)

    def forward(self, x, mask=None):
        """
        x: (batch_size, n_features, D)
        mask: (batch_size, n_features) boolean mask (1: valid, 0: pad)
        returns: (batch_size, D) pooled embedding
        """
        # 1) Self-Attention
        attn_out, _ = self.attn(
            x, x, x,
            key_padding_mask=(~mask) if mask is not None else None
        )
        x2 = self.ln1(x + attn_out)  # residual + layernorm

        # 2) Feedâ€forward
        ff_out = self.ff(x2)
        x3 = self.ln2(x2 + ff_out)   # residual + layernorm

        # 3) Pooling: â€œê° feature í† í°ë“¤ì˜ í‰ê· â€
        if mask is not None:
            mask_f = mask.unsqueeze(-1).float()  # (batch_size, n_features, 1)
            x3 = x3 * mask_f
            sum_x3 = x3.sum(dim=1)               # (batch_size, D)
            denom = mask_f.sum(dim=1).clamp(min=1.0)  # (batch_size, 1)
            pooled = sum_x3 / denom
        else:
            pooled = x3.mean(dim=1)  # (batch_size, D)

        return pooled  # (batch_size, D)


class TabularSetEmbedder(nn.Module):
    """
    Setâ€Transformer ê¸°ë°˜ Tabular Embedding:
      - all_feature_names: ê³ ì •ëœ â€œí•©ì§‘í•©â€ feature ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸
      - forward: feature_values (batch_size, n_features) â†’ h (batch_size, D)
    """
    def __init__(self, all_feature_names: list, d_name=16, d_val=16,
                 n_heads=4, dim_ff=64, dropout=0.1):
        super().__init__()
        self.embedder = FeatureEmbedding(all_feature_names,
                                         d_name=d_name,
                                         d_val=d_val)
        D = d_name + d_val
        self.encoder = SetTransformerEncoder(D,
                                             n_heads=n_heads,
                                             dim_ff=dim_ff,
                                             dropout=dropout)
        self.out_dim = D
        self.all_feature_names = all_feature_names

    def forward(self, feature_values: torch.Tensor):
        """
        feature_values: Tensor, shape=(batch_size, n_features)
        returns: Tensor, shape=(batch_size, D)
        """
        batch_size, n_features = feature_values.size()
        # mask: ëª¨ë“  featureê°€ validí•˜ë‹¤ê³  ê°€ì • â†’ ëª¨ë‘ True
        mask = torch.ones((batch_size, n_features),
                          dtype=torch.bool,
                          device=feature_values.device)

        feature_names = self.all_feature_names  # ê³ ì •ëœ ìˆœì„œ
        token_emb = self.embedder(feature_names, feature_values)
        h = self.encoder(token_emb, mask=mask)  # (batch_size, D)
        return h


# ===============================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •
# ===============================
print("ğŸš€ ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ XGBoost ìˆœì°¨ í•™ìŠµ ì‹œì‘")
print("="*70)

# ===============================
# 2. ë°ì´í„° ë¡œë“œ ë° ìë™ ë¶„ì„ (ì„ë² ë”© ë²„ì „)
# ===============================
def load_and_analyze_data_embedding(sentiment_file, target_file):
    """ë°ì´í„° ë¡œë“œ ë° êµ¬ì¡° ìë™ ë¶„ì„ (ì„ë² ë”© ë²„ì „)"""

    sentiment_df = pd.read_csv(sentiment_file)
    target_df = pd.read_csv(target_file)

    print(f"ğŸ“Š ì…ë ¥ ë°ì´í„° í˜•íƒœ: {sentiment_df.shape}")
    print(f"ğŸ“Š íƒ€ê²Ÿ ë°ì´í„° í˜•íƒœ: {target_df.shape}")

    # ì…ë ¥ ë°ì´í„° êµ¬ì¡° ê²€ì¦
    required_sentiment_cols = ['final_cluster', 'negative', 'positive', 'time_label']
    missing_sentiment_cols = [
        col for col in required_sentiment_cols
        if col not in sentiment_df.columns
    ]
    if missing_sentiment_cols:
        raise ValueError(f"ì…ë ¥ ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_sentiment_cols}")

    # íƒ€ê²Ÿ ë°ì´í„°ì—ì„œ í›„ë³´ì ìë™ ì¶”ì¶œ
    target_columns = [col for col in target_df.columns if col != 'time_label']
    if 'time_label' not in target_df.columns:
        raise ValueError("íƒ€ê²Ÿ ë°ì´í„°ì— 'time_label' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"\nğŸ“‹ ê°ì§€ëœ í›„ë³´ì: {target_columns}")
    print(f"ğŸ“‹ ì…ë ¥ ë°ì´í„° time_label: {sorted(sentiment_df['time_label'].unique())}")
    print(f"ğŸ“‹ íƒ€ê²Ÿ ë°ì´í„° time_label: {sorted(target_df['time_label'].unique())}")

    # ë°ì´í„° ê°€ìš©ì„± ë¶„ì„
    available_target_labels = sorted(target_df['time_label'].unique())
    all_input_labels = sorted(sentiment_df['time_label'].unique())
    missing_target_labels = [
        label for label in all_input_labels
        if label not in available_target_labels
    ]

    print(f"\nğŸ“Š ë°ì´í„° í˜„í™©:")
    print(f"   í•™ìŠµ ê°€ëŠ¥ time_label: {available_target_labels}")
    print(f"   ì˜ˆì¸¡ ëŒ€ìƒ time_label: {missing_target_labels}")

    return sentiment_df, target_df, target_columns, available_target_labels, missing_target_labels


# ===============================
# 3. ì¼ë°˜í™”ëœ í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ìƒì„± (KcELECTRA)
# ===============================
def create_kcelectra_embeddings(df, embedding_dim=768):
    """KcELECTRAë¥¼ í™œìš©í•œ í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ìƒì„±"""

    print(f"\nğŸ”§ KcELECTRA ê¸°ë°˜ í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ìƒì„±")

    model_name = "beomi/KcELECTRA-base-v2022"
    tokenizer = ElectraTokenizer.from_pretrained(model_name)
    model = ElectraModel.from_pretrained(model_name)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    model.eval()

    unique_clusters = df['final_cluster'].unique()
    print(f"   ğŸ“Š ì´ ê³ ìœ  í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(unique_clusters)}")

    cluster_texts = []
    for cluster in unique_clusters:
        if pd.isna(cluster):
            cluster_texts.append("ì•Œ ìˆ˜ ì—†ìŒ")
        else:
            processed_text = str(cluster).replace('_', ' ê´€ë ¨ ')
            cluster_texts.append(processed_text)

    embeddings = []
    with torch.no_grad():
        for text in cluster_texts:
            inputs = tokenizer(
                text,
                return_tensors='pt',
                max_length=128,
                truncation=True,
                padding=True
            ).to(device)
            outputs = model(**inputs)
            cls_embedding = outputs.last_hidden_state[:, 0, :].cpu().numpy()
            embeddings.append(cls_embedding[0])

    embeddings = np.array(embeddings)

    if embedding_dim < embeddings.shape[1]:
        pca = PCA(n_components=embedding_dim)
        embeddings = pca.fit_transform(embeddings)
        print(f"   ğŸ“ˆ PCA ì„¤ëª… ë¶„ì‚° ë¹„ìœ¨: {pca.explained_variance_ratio_.sum():.3f}")

    embedding_dict = {
        cluster: embeddings[i]
        for i, cluster in enumerate(unique_clusters)
    }

    print(f"   âœ… KcELECTRA ì„ë² ë”© ìƒì„± ì™„ë£Œ: {embeddings.shape}")
    return embedding_dict, tokenizer, model


# ===============================
# 4. ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ ê°ì • ë°ì´í„° ì§‘ê³„
# ===============================
def extract_candidate_from_cluster_general(cluster_name, candidate_list):
    """ì¼ë°˜í™”ëœ í›„ë³´ì ì¶”ì¶œ"""
    if pd.isna(cluster_name):
        return 'unknown'
    for candidate in candidate_list:
        if candidate in str(cluster_name):
            return candidate
    return 'other'


def aggregate_sentiment_with_embeddings_general(df, embedding_dict, candidate_list):
    """ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ ê°ì • ë°ì´í„° ì§‘ê³„"""

    print(f"\nğŸ”§ ì„ë² ë”© ê¸°ë°˜ ê°ì • ë°ì´í„° ì§‘ê³„ (í›„ë³´ì: {candidate_list})")

    aggregated_data = []
    embedding_dim = len(next(iter(embedding_dict.values())))

    for time_label in df['time_label'].unique():
        time_data = df[df['time_label'] == time_label]

        for candidate in candidate_list:
            candidate_clusters = []
            for _, row in time_data.iterrows():
                cluster_candidate = extract_candidate_from_cluster_general(
                    row['final_cluster'], candidate_list
                )
                if cluster_candidate == candidate:
                    candidate_clusters.append(row)

            if candidate_clusters:
                total_positive = sum([row['positive'] for row in candidate_clusters])
                total_negative = sum([row['negative'] for row in candidate_clusters])
                cluster_count = len(candidate_clusters)

                avg_positive = total_positive / cluster_count
                avg_negative = total_negative / cluster_count
                sentiment_score = avg_positive - avg_negative

                embeddings = []
                for row in candidate_clusters:
                    if row['final_cluster'] in embedding_dict:
                        embeddings.append(embedding_dict[row['final_cluster']])

                if embeddings:
                    avg_embedding = np.mean(embeddings, axis=0)
                else:
                    avg_embedding = np.zeros(embedding_dim)

                result = {
                    'time_label': time_label,
                    'candidate': candidate,
                    'avg_positive': avg_positive,
                    'avg_negative': avg_negative,
                    'sentiment_score': sentiment_score,
                    'cluster_count': cluster_count
                }

                for i in range(embedding_dim):
                    result[f'embedding_{i}'] = avg_embedding[i]

                aggregated_data.append(result)

            else:
                result = {
                    'time_label': time_label,
                    'candidate': candidate,
                    'avg_positive': 0.5,
                    'avg_negative': 0.5,
                    'sentiment_score': 0.0,
                    'cluster_count': 0
                }

                for i in range(embedding_dim):
                    result[f'embedding_{i}'] = 0.0

                aggregated_data.append(result)

    result_df = pd.DataFrame(aggregated_data)
    print(f"   âœ… ì§‘ê³„ ì™„ë£Œ: {result_df.shape}")
    return result_df

# ===============================
# 5. ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# ===============================
def create_embedding_features_general(agg_df, candidate_list):
    """ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ í”¼ì²˜ ìƒì„±"""

    print(f"\nğŸ”§ ì„ë² ë”© ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (í›„ë³´ì {len(candidate_list)}ëª…)")

    features_data = []
    embedding_dim = len([col for col in agg_df.columns if col.startswith('embedding_')])

    for time_label in agg_df['time_label'].unique():
        time_data = agg_df[agg_df['time_label'] == time_label]

        features = {'time_label': time_label}

        for candidate in candidate_list:
            candidate_data = time_data[time_data['candidate'] == candidate].iloc[0]

            features[f'{candidate}_positive'] = candidate_data['avg_positive']
            features[f'{candidate}_negative'] = candidate_data['avg_negative']
            features[f'{candidate}_sentiment'] = candidate_data['sentiment_score']
            features[f'{candidate}_cluster_count'] = candidate_data['cluster_count']

            for i in range(embedding_dim):
                features[f'{candidate}_embedding_{i}'] = candidate_data[f'embedding_{i}']

        sentiments = [features[f'{c}_sentiment'] for c in candidate_list]
        total_sentiment = sum(sentiments)
        if total_sentiment != 0:
            for i, candidate in enumerate(candidate_list):
                features[f'{candidate}_relative_sentiment'] = sentiments[i] / total_sentiment
        else:
            for candidate in candidate_list:
                features[f'{candidate}_relative_sentiment'] = 1.0 / len(candidate_list)

        embeddings = {}
        for candidate in candidate_list:
            embedding = np.array([
                features[f'{candidate}_embedding_{i}']
                for i in range(embedding_dim)
            ])
            embeddings[candidate] = embedding

        for i, cand1 in enumerate(candidate_list):
            for j, cand2 in enumerate(candidate_list):
                if i < j:
                    sim = cosine_similarity(
                        [embeddings[cand1]], [embeddings[cand2]]
                    )[0][0]
                    features[f'similarity_{cand1}_{cand2}'] = sim

        for candidate in candidate_list:
            embedding_norm = np.linalg.norm(embeddings[candidate])
            features[f'{candidate}_embedding_strength'] = embedding_norm

        all_embeddings = np.array(list(embeddings.values()))
        features['embedding_mean'] = np.mean(all_embeddings)
        features['embedding_std'] = np.std(all_embeddings)
        features['embedding_max'] = np.max(all_embeddings)
        features['embedding_min'] = np.min(all_embeddings)

        features_data.append(features)

    result_df = pd.DataFrame(features_data)
    print(f"   âœ… í”¼ì²˜ ìƒì„± ì™„ë£Œ: {result_df.shape}")
    return result_df


# ===============================
# 6. ì¼ë°˜í™”ëœ XGBoost í´ë˜ìŠ¤
# ===============================
class GeneralSequentialMultiOutputXGBoost:
    def __init__(self, target_columns, **params):
        self.target_columns = target_columns
        self.models = {}
        self.params = params
        self.is_fitted = False

    def fit(self, X, y):
        """ì´ˆê¸° í•™ìŠµ"""
        print("   ğŸ”§ ìƒˆë¡œìš´ ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ...")
        for col in self.target_columns:
            if col in y.columns:
                model = xgb.XGBRegressor(**self.params)
                model.fit(X, y[col])
                self.models[col] = model
        self.is_fitted = True

    def partial_fit(self, X, y):
        """ê¸°ì¡´ ëª¨ë¸ì— ì¦ë¶„ í•™ìŠµ"""
        if not self.is_fitted:
            return self.fit(X, y)

        print("   ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì— ì¦ë¶„ í•™ìŠµ ìˆ˜í–‰...")
        for col in self.target_columns:
            if col in y.columns and col in self.models:
                existing_booster = self.models[col].get_booster()
                new_model = xgb.XGBRegressor(**self.params)
                new_model.fit(X, y[col], xgb_model=existing_booster)
                self.models[col] = new_model

    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        predictions = {}
        for col in self.target_columns:
            if col in self.models:
                predictions[col] = self.models[col].predict(X)
            else:
                predictions[col] = np.full(len(X), 1.0 / len(self.target_columns))

        return pd.DataFrame(predictions, index=X.index)

# ===============================
# 7. ì¼ë°˜í™”ëœ ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
# ===============================
def run_general_embedding_pipeline(sentiment_file, target_file, embedding_dim=10):
    """ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ ì‹¤í–‰"""

    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
        sentiment_df, target_df, target_columns, available_target_labels, missing_target_labels = \
            load_and_analyze_data_embedding(sentiment_file, target_file)

        # 2. í´ëŸ¬ìŠ¤í„° ì„ë² ë”© ìƒì„± (KcELECTRA)
        embedding_dict, tokenizer, model = create_kcelectra_embeddings(
            sentiment_df, embedding_dim=embedding_dim
        )

        # 3. ì„ë² ë”© ê¸°ë°˜ ê°ì • ë°ì´í„° ì§‘ê³„
        agg_sentiment = aggregate_sentiment_with_embeddings_general(
            sentiment_df, embedding_dict, target_columns
        )

        # 4. ì„ë² ë”© ê¸°ë°˜ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        features_df = create_embedding_features_general(agg_sentiment, target_columns)

        # 5. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ê°„ì†Œí™”ëœ ë²„ì „)
        best_params = {
            'n_estimators': 50,
            'learning_rate': 0.1,
            'max_depth': 4,
            'subsample': 0.8,
            'colsample_bytree': 0.8
        }

        # 6. Setâ€Transformer ì„ë² ë” ì´ˆê¸°í™”
        #    features_df.columns ì¤‘ 'time_label' ì œì™¸
        all_feature_names = [col for col in features_df.columns if col != 'time_label']
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        set_embedder = TabularSetEmbedder(
            all_feature_names=all_feature_names,
            d_name=16,
            d_val=16,
            n_heads=4,
            dim_ff=64,
            dropout=0.1
        ).to(device)

        # 7. ìµœì¢… ëª¨ë¸ í•™ìŠµ (XGBoost + Setâ€Transformer ì„ë² ë”©)
        print(f"\n{'='*70}")
        print("ğŸ† ìµœì¢… ëª¨ë¸ í•™ìŠµ (ì„ë² ë”© + XGBoost)")
        print(f"{'='*70}")

        final_model = GeneralSequentialMultiOutputXGBoost(target_columns, **best_params)

        for epoch in available_target_labels:
            print(f"\nğŸ“š Epoch {epoch} í•™ìŠµ ì¤‘...")

            # 7-1. ë°ì´í„° ì¤€ë¹„
            current_features = features_df[features_df['time_label'] == epoch]
            current_target = target_df[target_df['time_label'] == epoch]
            current_merged = pd.merge(
                current_features, current_target,
                on='time_label', how='inner'
            )

            if len(current_merged) > 0:
                feature_columns = [
                    col for col in current_features.columns
                    if col != 'time_label'
                ]
                X_epoch = current_merged[feature_columns].reset_index(drop=True)
                y_epoch = current_merged[target_columns].reset_index(drop=True)

                print(f"   ğŸ“Š ì›ë³¸ X_epoch shape: {X_epoch.shape}")

                # 7-2. Setâ€Transformer ì„ë² ë”© ìƒì„±
                X_vals = torch.tensor(
                    X_epoch.values, dtype=torch.float32, device=device
                )  # (batch_size, n_features)
                with torch.no_grad():
                    emb_epoch = set_embedder(X_vals).cpu().numpy()  # (batch_size, D)

                emb_cols = [f'set_emb_{i}' for i in range(emb_epoch.shape[1])]
                df_emb = pd.DataFrame(emb_epoch, columns=emb_cols)

                X_epoch_emb = pd.concat(
                    [X_epoch.reset_index(drop=True),
                     df_emb.reset_index(drop=True)],
                    axis=1
                )
                print(f"   ğŸ”— X_epoch_emb shape (with set-emb): {X_epoch_emb.shape}")

                # 7-3. XGBoost í•™ìŠµ
                if not final_model.is_fitted:
                    final_model.fit(X_epoch_emb, y_epoch)
                    final_model.is_fitted = True
                    print("   âœ… XGBoost ìƒˆ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
                else:
                    final_model.partial_fit(X_epoch_emb, y_epoch)
                    print("   ğŸ”„ XGBoost ëª¨ë¸ ì¦ë¶„ í•™ìŠµ(Partial Fit) ì™„ë£Œ")

                # 7-4. Epochë³„ ì„±ëŠ¥ í‰ê°€
                predictions = final_model.predict(X_epoch_emb)
                predictions_normalized = predictions.div(
                    predictions.sum(axis=1), axis=0
                ) * 100
                y_norm = y_epoch.div(y_epoch.sum(axis=1), axis=0) * 100

                print(f"   ğŸ“Š Epoch {epoch} ì˜ˆì¸¡ ê²°ê³¼:")
                for candidate in target_columns:
                    actual = y_norm[candidate].iloc[0]
                    pred = predictions_normalized[candidate].iloc[0]
                    mae = abs(actual - pred)
                    print(f"      {candidate}: ì‹¤ì œ={actual:.2f}%, ì˜ˆì¸¡={pred:.2f}%, MAE={mae:.2f}%")

                overall_mae = mean_absolute_error(
                    y_norm.values.flatten(),
                    predictions_normalized.values.flatten()
                )
                print(f"   ğŸ¯ Epoch {epoch} ì „ì²´ MAE: {overall_mae:.2f}%")

        # 8. ê²°ì¸¡ëœ time_labelë“¤ì— ëŒ€í•œ ì˜ˆì¸¡
        prediction_results = {}
        if missing_target_labels:
            print(f"\n{'='*70}")
            print(f"ğŸ”® ê²°ì¸¡ time_label ì˜ˆì¸¡: {missing_target_labels}")
            print(f"{'='*70}")

            full_features = features_df.copy()
            feat_vals = torch.tensor(
                full_features.drop(columns=['time_label']).values,
                dtype=torch.float32, device=device
            )
            with torch.no_grad():
                emb_full = set_embedder(feat_vals).cpu().numpy()  # (N_all, D)

            df_emb_full = pd.DataFrame(
                emb_full, columns=[f'set_emb_{i}' for i in range(emb_full.shape[1])]
            )
            full_with_emb = pd.concat(
                [full_features.reset_index(drop=True).drop(columns=['time_label']),
                 df_emb_full.reset_index(drop=True)], axis=1
            )

            for time_label in missing_target_labels:
                mask = full_features['time_label'] == time_label
                if mask.sum() == 0:
                    print(f"   âš ï¸ time_label {time_label} ë°ì´í„° X")
                    continue

                X_pred = full_with_emb[mask.values].reset_index(drop=True)
                preds = final_model.predict(X_pred)
                preds_norm = preds.div(preds.sum(axis=1), axis=0) * 100

                print(f"\n   ğŸ¯ time_label {time_label} ì˜ˆì¸¡ ì§€ì§€ìœ¨:")
                for candidate in target_columns:
                    val = preds_norm[candidate].iloc[0]
                    print(f"      {candidate}: {val:.2f}%")
                prediction_results[time_label] = preds_norm.iloc[0]

        # 9. ê²°ê³¼ ìš”ì•½
        print(f"\n{'='*70}")
        print("ğŸ“Š ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print(f"{'='*70}")
        print(f"ğŸ† ì‚¬ìš©ëœ íŒŒë¼ë¯¸í„°: {best_params}")
        print(f"ğŸ“š í•™ìŠµì— ì‚¬ìš©ëœ time_label: {available_target_labels}")
        print(f"ğŸ”® ì˜ˆì¸¡ëœ time_label: {missing_target_labels}")
        print(f"ğŸ¯ ì„ë² ë”© ì°¨ì›: {embedding_dim} + SetTransformer({set_embedder.out_dim})")
        if missing_target_labels:
            print(f"\nğŸ—³ï¸ ì˜ˆì¸¡ ê²°ê³¼:")
            for time_label, predictions in prediction_results.items():
                print(f"   time_label {time_label}:")
                for candidate in target_columns:
                    val = predictions[candidate]
                    print(f"      {candidate}: {val:.2f}%")

        print(f"\nâœ… ì¼ë°˜í™”ëœ ì„ë² ë”© ê¸°ë°˜ ì˜ˆì¸¡ ëª¨ë¸ ì™„ë£Œ!")
        print("="*70)

        return final_model, prediction_results, embedding_dict

    except Exception as e:
        print(f"âŒ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None, None

if __name__ == "__main__":
    # sentiment_file, target_file ê²½ë¡œë¥¼ ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •
    sentiment_path = "cluster_sentiment_summary_final_21ëŒ€.csv"
    target_path = "weighted_average_21ëŒ€.csv"

    # embedding_dim ì„¤ì •
    embedding_dim = 10

    # í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ìµœì¢… ëª¨ë¸ê³¼ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ì–»ìŒ
    final_model, prediction_results, embedding_dict = run_general_embedding_pipeline(
        sentiment_path,
        target_path,
        embedding_dim=embedding_dim
    )

    if final_model is None:
        print("âŒ ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨. ì—ëŸ¬ ë©”ì‹œì§€ í™•ì¸")
    else:
        print("\nâœ… íŒŒì´í”„ë¼ì¸ì´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œ")
        print("prediction_results ì˜ˆì‹œ:", prediction_results)