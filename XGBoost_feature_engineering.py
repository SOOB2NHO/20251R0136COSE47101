import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import ParameterGrid
import warnings
warnings.filterwarnings('ignore')

# ===============================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ì„¤ì •
# ===============================
print("ğŸš€ ì¼ë°˜í™”ëœ XGBoost ìˆœì°¨ í•™ìŠµ ì‹œì‘")
print("="*70)

# ===============================
# 2. ë°ì´í„° ë¡œë“œ ë° ìë™ ë¶„ì„
# ===============================
def load_and_analyze_data(sentiment_file, target_file):
    """ë°ì´í„° ë¡œë“œ ë° êµ¬ì¡° ìë™ ë¶„ì„"""

    # ë°ì´í„° ë¡œë“œ
    sentiment_df = pd.read_csv(sentiment_file)
    target_df = pd.read_csv(target_file)

    print(f"ğŸ“Š ì…ë ¥ ë°ì´í„° í˜•íƒœ: {sentiment_df.shape}")
    print(f"ğŸ“Š íƒ€ê²Ÿ ë°ì´í„° í˜•íƒœ: {target_df.shape}")

    # ì…ë ¥ ë°ì´í„° êµ¬ì¡° ë¶„ì„
    required_sentiment_cols = ['final_cluster', 'negative', 'positive', 'time_label']
    missing_sentiment_cols = [col for col in required_sentiment_cols if col not in sentiment_df.columns]
    if missing_sentiment_cols:
        raise ValueError(f"ì…ë ¥ ë°ì´í„°ì— í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤: {missing_sentiment_cols}")

    # íƒ€ê²Ÿ ë°ì´í„° êµ¬ì¡° ë¶„ì„ (time_labelì„ ì œì™¸í•œ ë‚˜ë¨¸ì§€ê°€ í›„ë³´ìëª…)
    target_columns = [col for col in target_df.columns if col != 'time_label']
    if 'time_label' not in target_df.columns:
        raise ValueError("íƒ€ê²Ÿ ë°ì´í„°ì— 'time_label' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

    print(f"\nğŸ“‹ ê°ì§€ëœ í›„ë³´ì: {target_columns}")
    print(f"ğŸ“‹ ì…ë ¥ ë°ì´í„° time_label: {sorted(sentiment_df['time_label'].unique())}")
    print(f"ğŸ“‹ íƒ€ê²Ÿ ë°ì´í„° time_label: {sorted(target_df['time_label'].unique())}")

    # ë°ì´í„° ê°€ìš©ì„± ë¶„ì„
    available_target_labels = sorted(target_df['time_label'].unique())
    all_input_labels = sorted(sentiment_df['time_label'].unique())
    missing_target_labels = [label for label in all_input_labels if label not in available_target_labels]

    print(f"\nğŸ“Š ë°ì´í„° í˜„í™©:")
    print(f"   í•™ìŠµ ê°€ëŠ¥ time_label: {available_target_labels}")
    print(f"   ì˜ˆì¸¡ ëŒ€ìƒ time_label: {missing_target_labels}")

    return sentiment_df, target_df, target_columns, available_target_labels, missing_target_labels

# ===============================
# 3. í›„ë³´ì ìë™ ì¶”ì¶œ ë° ê°ì • ë°ì´í„° ì§‘ê³„
# ===============================
def extract_candidate_from_cluster(cluster_name, candidate_list):
    """í´ëŸ¬ìŠ¤í„°ëª…ì—ì„œ í›„ë³´ìëª… ìë™ ì¶”ì¶œ"""
    if pd.isna(cluster_name):
        return 'unknown'

    # í›„ë³´ìëª…ì´ í´ëŸ¬ìŠ¤í„°ëª…ì— í¬í•¨ëœ ê²½ìš° ì¶”ì¶œ
    for candidate in candidate_list:
        if candidate in str(cluster_name):
            return candidate
    return 'other'

def aggregate_sentiment_by_time(df, candidate_list):
    """time_labelë³„ë¡œ í›„ë³´ì ê°ì • ë°ì´í„° ì§‘ê³„ (ì¼ë°˜í™” ë²„ì „)"""

    print(f"\nğŸ”§ ê°ì • ë°ì´í„° ì§‘ê³„ (í›„ë³´ì: {candidate_list})")

    # í›„ë³´ì ì¶”ì¶œ
    df['candidate'] = df['final_cluster'].apply(
        lambda x: extract_candidate_from_cluster(x, candidate_list)
    )

    aggregated_data = []

    for time_label in df['time_label'].unique():
        time_data = df[df['time_label'] == time_label]

        for candidate in candidate_list:
            candidate_data = time_data[time_data['candidate'] == candidate]

            if len(candidate_data) > 0:
                total_mentions = len(candidate_data)
                avg_positive = candidate_data['positive'].mean()
                avg_negative = candidate_data['negative'].mean()
                sentiment_score = avg_positive - avg_negative

                aggregated_data.append({
                    'time_label': time_label,
                    'candidate': candidate,
                    'avg_positive': avg_positive,
                    'avg_negative': avg_negative,
                    'sentiment_score': sentiment_score,
                    'mention_count': total_mentions
                })
            else:
                # í•´ë‹¹ ì‹œì ì— ë°ì´í„°ê°€ ì—†ëŠ” ê²½ìš° ì¤‘ë¦½ê°’ ì‚¬ìš©
                aggregated_data.append({
                    'time_label': time_label,
                    'candidate': candidate,
                    'avg_positive': 0.5,
                    'avg_negative': 0.5,
                    'sentiment_score': 0.0,
                    'mention_count': 0
                })

    result_df = pd.DataFrame(aggregated_data)
    print(f"   âœ… ì§‘ê³„ ì™„ë£Œ: {result_df.shape}")

    return result_df

# ===============================
# 4. ì¼ë°˜í™”ëœ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
# ===============================
def create_features_general(agg_df, candidate_list):
    """ì¼ë°˜í™”ëœ í”¼ì²˜ ìƒì„± (í›„ë³´ì ìˆ˜ì— ê´€ê³„ì—†ì´ ë™ì‘)"""

    print(f"\nğŸ”§ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (í›„ë³´ì {len(candidate_list)}ëª…)")

    features_data = []

    for time_label in agg_df['time_label'].unique():
        time_data = agg_df[agg_df['time_label'] == time_label]

        features = {'time_label': time_label}

        # ê° í›„ë³´ìë³„ ê¸°ë³¸ í”¼ì²˜
        for candidate in candidate_list:
            candidate_data = time_data[time_data['candidate'] == candidate].iloc[0]

            features[f'{candidate}_positive'] = candidate_data['avg_positive']
            features[f'{candidate}_negative'] = candidate_data['avg_negative']
            features[f'{candidate}_sentiment'] = candidate_data['sentiment_score']
            features[f'{candidate}_mentions'] = candidate_data['mention_count']

        # ìƒëŒ€ì  ê°ì • ì ìˆ˜ ê³„ì‚°
        sentiments = [features[f'{c}_sentiment'] for c in candidate_list]
        total_sentiment = sum(sentiments)

        if total_sentiment != 0:
            for i, candidate in enumerate(candidate_list):
                features[f'{candidate}_relative_sentiment'] = sentiments[i] / total_sentiment
        else:
            for candidate in candidate_list:
                features[f'{candidate}_relative_sentiment'] = 1.0 / len(candidate_list)

        # í›„ë³´ìê°„ ê°ì • ì°¨ì´ í”¼ì²˜ (ëª¨ë“  ì¡°í•©)
        for i, cand1 in enumerate(candidate_list):
            for j, cand2 in enumerate(candidate_list):
                if i < j:  # ì¤‘ë³µ ë°©ì§€
                    features[f'sentiment_gap_{cand1}_{cand2}'] = (
                        features[f'{cand1}_sentiment'] - features[f'{cand2}_sentiment']
                    )

        # ì „ì²´ ê°ì • í†µê³„
        features['total_positive'] = sum([features[f'{c}_positive'] for c in candidate_list])
        features['total_negative'] = sum([features[f'{c}_negative'] for c in candidate_list])
        features['total_mentions'] = sum([features[f'{c}_mentions'] for c in candidate_list])
        features['sentiment_variance'] = np.var(sentiments)

        features_data.append(features)

    result_df = pd.DataFrame(features_data)
    print(f"   âœ… í”¼ì²˜ ìƒì„± ì™„ë£Œ: {result_df.shape}")

    return result_df

# ===============================
# 5. ì¼ë°˜í™”ëœ XGBoost í´ë˜ìŠ¤
# ===============================
class GeneralSequentialMultiOutputXGBoost:
    def __init__(self, target_columns, **params):
        self.target_columns = target_columns
        self.models = {}
        self.params = params
        self.is_fitted = False

    def fit(self, X, y):
        """ì´ˆê¸° í•™ìŠµ"""
        print("   ğŸ”§ ìƒˆë¡œìš´ ëª¨ë¸ ì´ˆê¸°í™” ë° í•™ìŠµ...")
        for col in self.target_columns:
            if col in y.columns:
                model = xgb.XGBRegressor(**self.params)
                model.fit(X, y[col])
                self.models[col] = model
        self.is_fitted = True

    def partial_fit(self, X, y):
        """ê¸°ì¡´ ëª¨ë¸ì— ì¦ë¶„ í•™ìŠµ"""
        if not self.is_fitted:
            return self.fit(X, y)

        print("   ğŸ”„ ê¸°ì¡´ ëª¨ë¸ì— ì¦ë¶„ í•™ìŠµ ìˆ˜í–‰...")
        for col in self.target_columns:
            if col in y.columns and col in self.models:
                existing_booster = self.models[col].get_booster()
                new_model = xgb.XGBRegressor(**self.params)
                new_model.fit(X, y[col], xgb_model=existing_booster)
                self.models[col] = new_model

    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        predictions = {}
        for col in self.target_columns:
            if col in self.models:
                predictions[col] = self.models[col].predict(X)
            else:
                # ëª¨ë¸ì´ ì—†ëŠ” ê²½ìš° í‰ê· ê°’ìœ¼ë¡œ ì˜ˆì¸¡
                predictions[col] = np.full(len(X), 1.0 / len(self.target_columns))

        return pd.DataFrame(predictions, index=X.index)

# ===============================
# 6. ì¼ë°˜í™”ëœ ë°ì´í„° ì¤€ë¹„ ë° í‰ê°€ í•¨ìˆ˜
# ===============================
def prepare_epoch_data_general(features_df, target_df, epoch, target_columns):
    """ì¼ë°˜í™”ëœ epoch ë°ì´í„° ì¤€ë¹„"""
    current_features = features_df[features_df['time_label'] == epoch]
    current_target = target_df[target_df['time_label'] == epoch]

    if len(current_features) == 0 or len(current_target) == 0:
        return None

    current_merged = pd.merge(current_features, current_target, on='time_label', how='inner')

    if len(current_merged) == 0:
        return None

    feature_columns = [col for col in current_features.columns if col != 'time_label']

    X_epoch = current_merged[feature_columns]
    y_epoch = current_merged[target_columns]

    return X_epoch, y_epoch

def evaluate_epoch_performance_general(model, X, y, epoch, target_columns):
    """ì¼ë°˜í™”ëœ epochë³„ ì„±ëŠ¥ í‰ê°€"""
    predictions = model.predict(X)

    # ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œê°’ì„ 0-100 ë²”ìœ„ë¡œ ì •ê·œí™”
    predictions_normalized = predictions.div(predictions.sum(axis=1), axis=0) * 100
    y_normalized = y.div(y.sum(axis=1), axis=0) * 100

    print(f"   ğŸ“Š Epoch {epoch} ì˜ˆì¸¡ ê²°ê³¼:")
    for candidate in target_columns:
        if candidate in y_normalized.columns and candidate in predictions_normalized.columns:
            actual = y_normalized[candidate].iloc[0]
            predicted = predictions_normalized[candidate].iloc[0]
            mae = abs(actual - predicted)
            print(f"      {candidate}: ì‹¤ì œ={actual:.2f}%, ì˜ˆì¸¡={predicted:.2f}%, MAE={mae:.2f}%")

    # ì „ì²´ MAE ê³„ì‚°
    overall_mae = mean_absolute_error(y_normalized.values.flatten(),
                                     predictions_normalized.values.flatten())

    print(f"   ğŸ¯ Epoch {epoch} ì „ì²´ MAE: {overall_mae:.2f}%")

    return {
        'epoch': epoch,
        'predictions': predictions_normalized,
        'actual': y_normalized,
        'mae': overall_mae
    }

# ===============================
# 7. ì¼ë°˜í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
# ===============================
def hyperparameter_tuning_general(features_df, target_df, target_columns, available_labels):
    """ì¼ë°˜í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹"""

    small_param_grid = {
        'n_estimators': [30, 50],
        'learning_rate': [0.1, 0.15],
        'max_depth': [3, 4],
        'subsample': [0.8, 0.9],
        'colsample_bytree': [0.8, 0.9]
    }

    best_score = float('inf')
    best_params = None

    if len(available_labels) < 2:
        print("âš ï¸ êµì°¨ ê²€ì¦ì„ ìœ„í•œ ì¶©ë¶„í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        return {
            'n_estimators': 50,
            'learning_rate': 0.1,
            'max_depth': 4,
            'subsample': 0.8,
            'colsample_bytree': 0.8
        }

    total_combinations = len(list(ParameterGrid(small_param_grid)))

    print(f"\n{'='*70}")
    print("ğŸ”§ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")
    print(f"{'='*70}")
    print(f"ì‚¬ìš© ê°€ëŠ¥í•œ time_label: {available_labels}")
    print(f"ì´ {total_combinations}ê°œ ì¡°í•© í…ŒìŠ¤íŠ¸")

    for i, params in enumerate(ParameterGrid(small_param_grid)):
        print(f"\nğŸ§ª ì¡°í•© {i+1}/{total_combinations}: {params}")

        try:
            fold_scores = []

            for test_label in available_labels:
                train_labels = [label for label in available_labels if label != test_label]

                if not train_labels:
                    continue

                model = GeneralSequentialMultiOutputXGBoost(target_columns, **params)

                # í•™ìŠµ
                for epoch in train_labels:
                    epoch_data = prepare_epoch_data_general(features_df, target_df, epoch, target_columns)
                    if epoch_data is not None:
                        X_epoch, y_epoch = epoch_data
                        if epoch == train_labels[0]:
                            model.fit(X_epoch, y_epoch)
                        else:
                            model.partial_fit(X_epoch, y_epoch)

                # í…ŒìŠ¤íŠ¸
                test_data = prepare_epoch_data_general(features_df, target_df, test_label, target_columns)
                if test_data is not None:
                    X_test, y_test = test_data
                    predictions = model.predict(X_test)
                    predictions_normalized = predictions.div(predictions.sum(axis=1), axis=0) * 100
                    y_test_normalized = y_test.div(y_test.sum(axis=1), axis=0) * 100

                    mae = mean_absolute_error(y_test_normalized.values.flatten(),
                                            predictions_normalized.values.flatten())
                    fold_scores.append(mae)

            if fold_scores:
                avg_score = np.mean(fold_scores)
                print(f"   ğŸ“Š í‰ê·  MAE: {avg_score:.3f}%")

                if avg_score < best_score:
                    best_score = avg_score
                    best_params = params.copy()
                    print(f"   âœ¨ ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! MAE: {avg_score:.3f}%")

        except Exception as e:
            print(f"   âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            continue

    return best_params

# ===============================
# 8. ì¼ë°˜í™”ëœ ì˜ˆì¸¡ í•¨ìˆ˜
# ===============================
def predict_missing_time_labels_general(model, features_df, missing_labels, target_columns):
    """ì¼ë°˜í™”ëœ ê²°ì¸¡ time_label ì˜ˆì¸¡"""

    print(f"\n{'='*70}")
    print(f"ğŸ”® ê²°ì¸¡ time_label ì˜ˆì¸¡: {missing_labels}")
    print(f"{'='*70}")

    predictions_results = {}

    for time_label in missing_labels:
        predict_features = features_df[features_df['time_label'] == time_label]

        if len(predict_features) == 0:
            print(f"   âš ï¸ time_label {time_label} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            continue

        feature_columns = [col for col in predict_features.columns if col != 'time_label']
        X_predict = predict_features[feature_columns]

        print(f"   ğŸ“Š time_label {time_label} ì˜ˆì¸¡ ë°ì´í„° í¬ê¸°: {X_predict.shape}")

        # ì˜ˆì¸¡ ìˆ˜í–‰
        predictions = model.predict(X_predict)
        predictions_normalized = predictions.div(predictions.sum(axis=1), axis=0) * 100

        print(f"   ğŸ¯ time_label {time_label} ì˜ˆì¸¡ ì§€ì§€ìœ¨:")
        for candidate in target_columns:
            if candidate in predictions_normalized.columns:
                predicted_rate = predictions_normalized[candidate].iloc[0]
                print(f"      {candidate}: {predicted_rate:.2f}%")

        predictions_results[time_label] = predictions_normalized.iloc[0]

    return predictions_results

# ===============================
# 9. ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ì¼ë°˜í™”)
# ===============================
def run_general_pipeline(sentiment_file, target_file):
    """ì¼ë°˜í™”ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""

    try:
        # 1. ë°ì´í„° ë¡œë“œ ë° ë¶„ì„
        sentiment_df, target_df, target_columns, available_target_labels, missing_target_labels = load_and_analyze_data(
            sentiment_file, target_file
        )

        # 2. ê°ì • ë°ì´í„° ì§‘ê³„
        agg_sentiment = aggregate_sentiment_by_time(sentiment_df, target_columns)

        # 3. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        features_df = create_features_general(agg_sentiment, target_columns)

        # 4. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
        best_params = hyperparameter_tuning_general(
            features_df, target_df, target_columns, available_target_labels
        )

        if best_params is None:
            print("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤íŒ¨. ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì‚¬ìš©.")
            best_params = {
                'n_estimators': 50,
                'learning_rate': 0.1,
                'max_depth': 4,
                'subsample': 0.8,
                'colsample_bytree': 0.8
            }

        # 5. ìµœì¢… ëª¨ë¸ í•™ìŠµ
        print(f"\n{'='*70}")
        print("ğŸ† ìµœì¢… ëª¨ë¸ í•™ìŠµ")
        print(f"{'='*70}")

        final_model = GeneralSequentialMultiOutputXGBoost(target_columns, **best_params)

        for epoch in available_target_labels:
            print(f"\nğŸ“š Epoch {epoch} í•™ìŠµ ì¤‘...")
            epoch_data = prepare_epoch_data_general(features_df, target_df, epoch, target_columns)
            if epoch_data is not None:
                X_epoch, y_epoch = epoch_data
                print(f"   ğŸ“Š ë°ì´í„° í¬ê¸°: {X_epoch.shape}")

                if epoch == available_target_labels[0]:
                    final_model.fit(X_epoch, y_epoch)
                else:
                    final_model.partial_fit(X_epoch, y_epoch)

                evaluate_epoch_performance_general(final_model, X_epoch, y_epoch, epoch, target_columns)

        # 6. ê²°ì¸¡ëœ time_labelë“¤ì— ëŒ€í•œ ì˜ˆì¸¡
        prediction_results = {}
        if missing_target_labels:
            prediction_results = predict_missing_time_labels_general(
                final_model, features_df, missing_target_labels, target_columns
            )

        # 7. ê²°ê³¼ ìš”ì•½
        print(f"\n{'='*70}")
        print("ìµœì¢… ê²°ê³¼ ìš”ì•½")
        print(f"{'='*70}")
        print(f"ìµœì  íŒŒë¼ë¯¸í„°: {best_params}")
        print(f"í•™ìŠµì— ì‚¬ìš©ëœ time_label: {available_target_labels}")
        print(f"ì˜ˆì¸¡ëœ time_label: {missing_target_labels}")

        if missing_target_labels:
            print(f"\nğŸ—³ï¸ ì˜ˆì¸¡ ê²°ê³¼:")
            for time_label, predictions in prediction_results.items():
                print(f"   time_label {time_label}:")
                for candidate in target_columns:
                    if candidate in predictions:
                        print(f"      {candidate}: {predictions[candidate]:.2f}%")

        print(f"\nì¼ë°˜í™”ëœ ì˜ˆì¸¡ ëª¨ë¸ ì™„ë£Œ!")
        print("="*70)

        return final_model, prediction_results

    except Exception as e:
        print(f"ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return None, None

# ===============================
# 10. ì‚¬ìš© ì˜ˆì‹œ
# ===============================
if __name__ == "__main__":
    '''
    # ì‚¬ìš© ì˜ˆì‹œ 1: 20ëŒ€ ëŒ€ì„  ë°ì´í„°
    print("20ëŒ€ ëŒ€ì„  ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸")
    model_20, results_20 = run_general_pipeline(
        "cluster_sentiment_summary_final_20ëŒ€.csv",
        "weighted_average_20ëŒ€.csv"
    )
    '''

    # ì‚¬ìš© ì˜ˆì‹œ 2: 21ëŒ€ ëŒ€ì„  ë°ì´í„°
    print("ğŸ—³ï¸ 21ëŒ€ ëŒ€ì„  ë°ì´í„°ë¡œ í…ŒìŠ¤íŠ¸")
    model_21, results_21 = run_general_pipeline(
        "cluster_sentiment_summary_final_21ëŒ€.csv",
        "weighted_average_21ëŒ€.csv"
    )