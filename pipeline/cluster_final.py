import os
import re
import pandas as pd
import numpy as np
import torch
from itertools import product
from sentence_transformers import SentenceTransformer, models
from sklearn.metrics.pairwise import cosine_similarity

# ì²˜ë¦¬í•  CSV ëª©ë¡
csv_files = [
    "/content/2025_comment_0519_0521.csv",
    "/content/2025_comment_0524_0526.csv",
    "/content/2025_comment_0528_0530.csv"
]

# KcELECTRA SentenceTransformer ëª¨ë¸ ë¡œë“œ
hf_model = "beomi/KcELECTRA-base"
word_embedding_model = models.Transformer(hf_model, max_seq_length=128)
pooling_model = models.Pooling(
    word_embedding_model.get_word_embedding_dimension(),
    pooling_mode_mean_tokens=True,
    pooling_mode_cls_token=False,
    pooling_mode_max_tokens=False
)
model = SentenceTransformer(modules=[word_embedding_model, pooling_model], device="cuda" if torch.cuda.is_available() else "cpu")

# í›„ë³´ì ë¦¬ìŠ¤íŠ¸
candidates = ["ì´ì¬ëª…", "ê¹€ë¬¸ìˆ˜", "ì´ì¤€ì„"]
candidate_embs = model.encode(candidates, convert_to_numpy=True)

# íŒŒì¼ë³„ ì²˜ë¦¬
for csv_path in csv_files:
    print(f"\nğŸ“‚ Processing: {csv_path}")
    filename = os.path.basename(csv_path)
    date_match = re.search(r"\d{4}_\d{4}", filename)  # â† ë‚ ì§œ íŒ¨í„´ ìˆ˜ì •
    if not date_match:
        print("âŒ ë‚ ì§œ íŒ¨í„´ ì¸ì‹ ì‹¤íŒ¨")
        continue
    date_str = date_match.group()

    # í‚¤ì›Œë“œ íŒŒì¼ ë¡œë“œ
    txt_path = f"/content/{date_str}.txt"
    if not os.path.exists(txt_path):
        print(f"âŒ í‚¤ì›Œë“œ íŒŒì¼ ì—†ìŒ: {txt_path}")
        continue
    with open(txt_path, 'r', encoding='utf-8') as f:
        keywords = [line.strip().split(":")[-1].strip() for line in f if ":" in line]

    # ëŒ“ê¸€ ë°ì´í„° ë¡œë“œ
    df = pd.read_csv(csv_path, encoding='utf-8-sig')
    df = df.dropna(subset=['comment_text']).reset_index(drop=True)
    sentences = df['comment_text'].astype(str).tolist()

    # ëŒ“ê¸€ ì„ë² ë”©
    embeddings = model.encode(sentences, batch_size=64, show_progress_bar=True, convert_to_numpy=True)

    # í‚¤ì›Œë“œ ì„ë² ë”© ë° 1ì°¨ í´ëŸ¬ìŠ¤í„°
    keyword_embeddings = model.encode(keywords, batch_size=4, convert_to_numpy=True)
    sims = cosine_similarity(embeddings, keyword_embeddings)
    assigned = np.argmax(sims, axis=1)
    df['candidate_cluster'] = [keywords[i] for i in assigned]

    # 2ì°¨ í´ëŸ¬ìŠ¤í„°ë§ (ì‚¬ëŒ ì´ë¦„ì€ ê·¸ëŒ€ë¡œ, ë‚˜ë¨¸ì§€ëŠ” _ì •ì¹˜ì¸ëª… ì¶”ê°€)
    final_clusters = []
    for i, row in df.iterrows():
        topic = row['candidate_cluster']
        if topic in candidates:
            final_clusters.append(topic)
        else:
            emb = embeddings[i].reshape(1, -1)
            cand_sim = cosine_similarity(emb, candidate_embs)
            best_cand = candidates[np.argmax(cand_sim)]
            final_clusters.append(f"{topic}_{best_cand}")
    df['final_cluster'] = final_clusters

    # ê²°ê³¼ ì €ì¥
    output_path = f"/content/clustered_{date_str}.csv"
    df.to_csv(output_path, index=False, encoding='utf-8-sig')
    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

import glob

# ë¶„ì„ ëŒ€ìƒ íŒŒì¼ ëª©ë¡ (ì›í•˜ëŠ” ë²”ìœ„ë¡œ íŒ¨í„´ ìˆ˜ì • ê°€ëŠ¥)
clustered_files = glob.glob("/content/clustered_*.csv")

for path in clustered_files:
    print(f"\nğŸ“Š [ë¶„í¬ ë¶„ì„] íŒŒì¼: {path}")
    df = pd.read_csv(path, encoding='utf-8-sig')

    # í´ëŸ¬ìŠ¤í„° ë¶„í¬ ì¶œë ¥
    if 'candidate_cluster' in df.columns:
        cluster_counts = df['final_cluster'].value_counts().sort_values(ascending=False)
        print(cluster_counts)
    else:
        print("âš ï¸ 'candidate_cluster' ì—´ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")