import pandas as pd
import numpy as np
import os
import re
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
from sklearn.metrics.pairwise import cosine_similarity
import matplotlib.pyplot as plt
import seaborn as sns
from gensim import corpora, models
from gensim.models import CoherenceModel

class CompleteLDASystem:
    def __init__(self, text_column='comment_text', num_topics=10):
        """
        ì™„ì „í•œ LDA í† í”½ ëª¨ë¸ë§ ì‹œìŠ¤í…œ
        
        Parameters:
        text_column: í…ìŠ¤íŠ¸ê°€ í¬í•¨ëœ ì»¬ëŸ¼ëª…
        num_topics: ì¶”ì¶œí•  í† í”½ ìˆ˜
        """
        self.text_column = text_column
        self.num_topics = num_topics
        self.vectorizer = None
        self.doc_term_matrix = None
        self.lda_model = None
        self.feature_names = None
        self.gensim_dictionary = None
        self.gensim_corpus = None
        self.gensim_lda_model = None
        self.tokenized_texts = None

        # ì„±ëŠ¥ í‰ê°€ ê²°ê³¼ ì €ì¥
        self.evaluation_results = {}

    def load_and_preprocess_data(self, csv_files):
        """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬"""
        all_texts = []
        file_info = []

        for csv_path in csv_files:
            try:
                print(f"ë°ì´í„° ë¡œë“œ ì¤‘: {csv_path}")
                df = pd.read_csv(csv_path, encoding='utf-8')

                texts = df[self.text_column].dropna().drop_duplicates().tolist()
                texts = [text.strip() for text in texts if text.strip()]

                print(f"{len(texts):,}ê°œ ìœ íš¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ")

                for text in texts:
                    file_info.append({
                        'file_path': csv_path,
                        'text': text
                    })

                all_texts.extend(texts)

            except Exception as e:
                print(f"íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {csv_path}: {e}")
                continue

        print(f"ğŸ”„ ì´ {len(all_texts):,}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return all_texts, file_info

    def preprocess_text(self, texts, min_df=2, max_df=0.95, max_features=1000):
        """í…ìŠ¤íŠ¸ ë²¡í„°í™” ë° Gensim í˜•ì‹ ë³€í™˜"""
        print("í…ìŠ¤íŠ¸ ë²¡í„°í™” ì¤‘...")

        # 1. Scikit-learn CountVectorizer
        self.vectorizer = CountVectorizer(
            min_df=min_df,
            max_df=max_df,
            max_features=max_features,
            token_pattern=r'\b[ê°€-í£]{2,}\b',
            lowercase=False
        )

        self.doc_term_matrix = self.vectorizer.fit_transform(texts)
        self.feature_names = self.vectorizer.get_feature_names_out()

        print(f"ë²¡í„°í™” ì™„ë£Œ: {self.doc_term_matrix.shape[0]:,}ê°œ ë¬¸ì„œ, {self.doc_term_matrix.shape[1]:,}ê°œ ë‹¨ì–´")

        # 2. Gensim í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        print("ğŸ”„ Gensim í˜•ì‹ ë³€í™˜ ì¤‘...")

        self.tokenized_texts = []
        for text in texts:
            tokens = [word for word in text.split() if len(word) >= 2]
            self.tokenized_texts.append(tokens)

        # 3. Gensim ì‚¬ì „ ë° ì½”í¼ìŠ¤ ìƒì„±
        self.gensim_dictionary = corpora.Dictionary(self.tokenized_texts)
        self.gensim_dictionary.filter_extremes(no_below=min_df, no_above=max_df, keep_n=max_features)
        self.gensim_corpus = [self.gensim_dictionary.doc2bow(text) for text in self.tokenized_texts]

        print(f"ğŸ“š Gensim ì‚¬ì „ í¬ê¸°: {len(self.gensim_dictionary)}")

        return self.tokenized_texts

    def train_lda_model(self, max_iter=20, learning_method='online', random_state=42):
        """LDA ëª¨ë¸ í•™ìŠµ (Scikit-learn ë° Gensim ë²„ì „)"""
        print("LDA ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # 4. Scikit-learn LDA
        self.lda_model = LatentDirichletAllocation(
            n_components=self.num_topics,
            max_iter=max_iter,
            learning_method=learning_method,
            random_state=random_state,
            doc_topic_prior=1.0,
            topic_word_prior=0.1,
            n_jobs=-1
        )

        self.lda_model.fit(self.doc_term_matrix)

        # Gensim LDA (Coherence ê³„ì‚°ìš©)
        self.gensim_lda_model = models.LdaModel(
            corpus=self.gensim_corpus,
            id2word=self.gensim_dictionary,
            num_topics=self.num_topics,
            random_state=random_state,
            passes=10,
            alpha='auto',
            per_word_topics=True
        )

        print(f"LDA ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {self.num_topics}ê°œ í† í”½")

    def calculate_coherence_score(self):
        """í† í”½ ì¼ê´€ì„± ì ìˆ˜ ê³„ì‚°"""
        print("Coherence ì ìˆ˜ ê³„ì‚° ì¤‘...")

        try:
            coherence_model = CoherenceModel(
                model=self.gensim_lda_model,
                texts=self.tokenized_texts,
                dictionary=self.gensim_dictionary,
                coherence='c_v'
            )
            coherence_score = coherence_model.get_coherence()

            print(f"Coherence Score (C_V): {coherence_score:.4f}")
            return coherence_score

        except Exception as e:
            print(f"Coherence ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def calculate_topic_diversity(self, top_k=25):
        """í† í”½ ë‹¤ì–‘ì„± ê³„ì‚°"""
        print("Topic Diversity ê³„ì‚° ì¤‘...")

        try:
            topic_words = []
            for topic_idx in range(self.num_topics):
                top_words_idx = self.lda_model.components_[topic_idx].argsort()[-top_k:][::-1]
                topic_words.append(set(top_words_idx))

            unique_words = set()
            total_words = 0

            for words in topic_words:
                unique_words.update(words)
                total_words += len(words)

            diversity_score = len(unique_words) / total_words if total_words > 0 else 0

            print(f"Topic Diversity: {diversity_score:.4f}")
            return diversity_score

        except Exception as e:
            print(f"Topic Diversity ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def calculate_perplexity(self):
        """Perplexity ì ìˆ˜ ê³„ì‚°"""
        print("Perplexity ì ìˆ˜ ê³„ì‚° ì¤‘...")

        try:
            # 5. ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            perplexity_score = self.lda_model.perplexity(self.doc_term_matrix)
            gensim_perplexity = self.gensim_lda_model.log_perplexity(self.gensim_corpus)

            print(f"Perplexity (Scikit-learn): {perplexity_score:.4f}")
            print(f"Log Perplexity (Gensim): {gensim_perplexity:.4f}")

            return perplexity_score, gensim_perplexity

        except Exception as e:
            print(f"Perplexity ê³„ì‚° ì‹¤íŒ¨: {e}")
            return float('inf'), float('inf')

    def evaluate_model_performance(self):
        """ì¢…í•©ì ì¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print("\n=== LDA ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")

        coherence_score = self.calculate_coherence_score()
        diversity_score = self.calculate_topic_diversity()
        perplexity_sklearn, perplexity_gensim = self.calculate_perplexity()

        self.evaluation_results = {
            'coherence_score': coherence_score,
            'topic_diversity': diversity_score,
            'perplexity_sklearn': perplexity_sklearn,
            'perplexity_gensim': perplexity_gensim,
            'num_topics': self.num_topics
        }

        print(f"\n=== ì„±ëŠ¥ í‰ê°€ ìš”ì•½ ===")
        print(f"Coherence Score: {coherence_score:.4f}")
        print(f"Topic Diversity: {diversity_score:.4f}")
        print(f"Perplexity (Sklearn): {perplexity_sklearn:.4f}")
        print(f"Log Perplexity (Gensim): {perplexity_gensim:.4f}")

        return self.evaluation_results

    def display_topics(self, num_words=10):
        """í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶œë ¥"""
        print("\n=== í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ===")

        topics_info = []
        for topic_idx, topic in enumerate(self.lda_model.components_):
            top_words_idx = topic.argsort()[-num_words:][::-1]
            top_words = [self.feature_names[i] for i in top_words_idx]
            top_weights = [topic[i] for i in top_words_idx]

            print(f"\ní† í”½ {topic_idx + 1}:")
            for word, weight in zip(top_words, top_weights):
                print(f"   {word}: {weight:.4f}")

            topics_info.append({
                'topic_id': topic_idx + 1,
                'words': top_words,
                'weights': top_weights
            })

        return topics_info

    def get_document_topics(self, texts):
        """ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ ê³„ì‚°"""
        print("ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ê³„ì‚° ì¤‘...")

        doc_topic_probs = self.lda_model.transform(self.doc_term_matrix)

        document_topics = []
        for i, (text, doc_topics) in enumerate(zip(texts, doc_topic_probs)):
            dominant_topic = doc_topics.argmax()
            dominant_prob = doc_topics.max()

            document_topics.append({
                'document_id': i,
                'text': text,
                'dominant_topic': dominant_topic + 1,
                'probability': dominant_prob,
                'all_topic_probs': doc_topics.tolist()
            })

        return pd.DataFrame(document_topics)

def run_complete_lda_pipeline():
    """ì™„ì „í•œ LDA íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("=== ì™„ì „í•œ LDA í† í”½ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

    csv_files = [
        "comments_0204_0206.csv",
        "comments_0212_0214.csv",
        "comments_0222_0224.csv",
        "comments_0226_0228.csv",
        "comments_0303_0305.csv"
    ]

    existing_files = [f for f in csv_files if os.path.exists(f)]
    print(f"ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼: {len(existing_files)}ê°œ")

    if not existing_files:
        print("ì²˜ë¦¬í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # LDA ì‹œìŠ¤í…œ ê°ì²´ ìƒì„±
    lda_system = CompleteLDASystem(
        text_column='comment_text',
        num_topics=10
    )

    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    all_texts, file_info = lda_system.load_and_preprocess_data(existing_files)

    if not all_texts:
        print("ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. í…ìŠ¤íŠ¸ ë²¡í„°í™” ë° ì½”í¼ìŠ¤ ìƒì„±
    tokenized_texts = lda_system.preprocess_text(
        all_texts,
        min_df=3,
        max_df=0.8,
        max_features=1000
    )

    # 3. LDA ëª¨ë¸ í•™ìŠµ
    lda_system.train_lda_model(max_iter=20)

    # 4. ì„±ëŠ¥ í‰ê°€
    evaluation_results = lda_system.evaluate_model_performance()

    # 5. í† í”½ ë¶„ì„ ë° ê²°ê³¼ ì¶œë ¥
    topics_info = lda_system.display_topics(num_words=10)

    # 6. ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ê³„ì‚°
    document_topics_df = lda_system.get_document_topics(all_texts)

    print("\n=== ì™„ì „í•œ LDA í† í”½ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
    print(f"ìµœì¢… ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   Coherence: {evaluation_results['coherence_score']:.4f}")
    print(f"   Diversity: {evaluation_results['topic_diversity']:.4f}")
    print(f"   Perplexity: {evaluation_results['perplexity_sklearn']:.4f}")

    return lda_system, evaluation_results

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    try:
        import gensim
        print("Gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸")
    except ImportError:
        print("Gensim ì„¤ì¹˜ í•„ìš”")
        exit()

    lda_model, results = run_complete_lda_pipeline()
