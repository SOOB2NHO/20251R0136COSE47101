import pandas as pd
import numpy as np
import torch
import torch.nn as nn
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from sklearn.feature_extraction.text import CountVectorizer
from tqdm import tqdm
import os

# GPU í™˜ê²½ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

class PoliticalDomainHyperParameters:
    """ì •ì¹˜ ë„ë©”ì¸ íŠ¹í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°"""

    # ëª¨ë¸ ì„¤ì •
    MODEL_NAME = 'monologg/koelectra-base-v3-discriminator'
    MAX_LENGTH = 128
    BATCH_SIZE = 32

    # ê³„ì¸µì  í† í”½ êµ¬ì¡°
    POLITICAL_HIERARCHY = {
        'level1': ['yoon', 'lee', 'neutral'],
        'level2': ['policy', 'scandal', 'campaign', 'general'],
        'level3_topics_per_person': 3
    }

    # ETM íŒŒë¼ë¯¸í„°
    NUM_TOPICS_TOTAL = 12
    EMBEDDING_DIM = 256
    HIDDEN_DIM = 400
    DROPOUT = 0.5

    # ì–´íœ˜ ì„¤ì •
    MIN_DF = 3
    MAX_DF = 0.8
    MAX_FEATURES = 1000

class PoliticalDocumentClassifier:
    """ì •ì¹˜ì¸ë³„ ë¬¸ì„œ ë¶„ë¥˜ê¸°"""

    def __init__(self):
        self.person_keywords = {
            'yoon': ['ìœ¤ì„ì—´', 'ìœ¤', 'ëŒ€í†µë ¹', 'í˜„ì •ë¶€', 'ì •ë¶€', 'ì—¬ë‹¹', 'êµ­ë¯¼ì˜í˜'],
            'lee': ['ì´ì¬ëª…', 'ì´', 'ë¯¼ì£¼ë‹¹', 'ì•¼ë‹¹', 'ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹'],
            'neutral': []
        }

        self.issue_keywords = {
            'policy': ['ì •ì±…', 'ê³µì•½', 'ë²•ì•ˆ', 'ì œë„', 'ê°œí˜', 'ì˜ˆì‚°', 'ê²½ì œ', 'ë³µì§€'],
            'scandal': ['ìˆ˜ì‚¬', 'ê²€ì°°', 'ì¬íŒ', 'í˜ì˜', 'ë¹„ë¦¬', 'ë…¼ë€', 'ì˜í˜¹'],
            'campaign': ['ì„ ê±°', 'ìº í˜ì¸', 'ìœ ì„¸', 'ê³µì²œ', 'í›„ë³´', 'ì§€ì§€ìœ¨'],
            'general': ['ë°œì–¸', 'íšŒê²¬', 'ì„±ëª…', 'ì…ì¥', 'ë°˜ì‘', 'ë¹„íŒ']
        }

    def classify_documents(self, texts):
        """ë¬¸ì„œë¥¼ ì¸ë¬¼ë³„, ì´ìŠˆë³„ë¡œ ë¶„ë¥˜"""
        person_labels = []
        issue_labels = []

        for text in tqdm(texts, desc="ë¬¸ì„œ ë¶„ë¥˜ ì¤‘"):
            text_lower = text.lower()

            # ì¸ë¬¼ ë¶„ë¥˜
            person_scores = {}
            for person, keywords in self.person_keywords.items():
                if person == 'neutral':
                    continue
                score = sum(text_lower.count(keyword) for keyword in keywords)
                person_scores[person] = score

            if max(person_scores.values()) > 0:
                dominant_person = max(person_scores, key=person_scores.get)
            else:
                dominant_person = 'neutral'

            person_labels.append(dominant_person)

            # ì´ìŠˆ ë¶„ë¥˜
            issue_scores = {}
            for issue, keywords in self.issue_keywords.items():
                score = sum(text_lower.count(keyword) for keyword in keywords)
                issue_scores[issue] = score

            if max(issue_scores.values()) > 0:
                dominant_issue = max(issue_scores, key=issue_scores.get)
            else:
                dominant_issue = 'general'

            issue_labels.append(dominant_issue)

        return person_labels, issue_labels

    def get_classification_summary(self, person_labels, issue_labels):
        """ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½"""
        person_counts = {}
        issue_counts = {}
        
        for person in person_labels:
            person_counts[person] = person_counts.get(person, 0) + 1
            
        for issue in issue_labels:
            issue_counts[issue] = issue_counts.get(issue, 0) + 1
            
        return person_counts, issue_counts

class KoELECTRAEmbedding:
    """KoELECTRA ì„ë² ë”© ëª¨ë¸"""

    def __init__(self, model_name=PoliticalDomainHyperParameters.MODEL_NAME):
        print(f"KoELECTRA ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

        if torch.cuda.is_available():
            self.model = self.model.to(device)

        print("KoELECTRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def encode_documents(self, texts, batch_size=PoliticalDomainHyperParameters.BATCH_SIZE):
        """ë¬¸ì„œ ì„ë² ë”© ìƒì„±"""
        embeddings = []
        self.model.eval()

        with torch.no_grad():
            for i in tqdm(range(0, len(texts), batch_size), desc="ë¬¸ì„œ ì„ë² ë”© ìƒì„±"):
                batch = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=PoliticalDomainHyperParameters.MAX_LENGTH,
                    return_tensors='pt'
                )

                if torch.cuda.is_available():
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                outputs = self.model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_embeddings)

        return np.vstack(embeddings)

    def encode_words(self, words, batch_size=PoliticalDomainHyperParameters.BATCH_SIZE):
        """ë‹¨ì–´ ì„ë² ë”© ìƒì„±"""
        embeddings = []
        self.model.eval()

        with torch.no_grad():
            for i in tqdm(range(0, len(words), batch_size), desc="ë‹¨ì–´ ì„ë² ë”© ìƒì„±"):
                batch = words[i:i+batch_size]
                inputs = self.tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=32,
                    return_tensors='pt'
                )

                if torch.cuda.is_available():
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                outputs = self.model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_embeddings)

        return np.vstack(embeddings)

class ETMPreprocessor:
    """ETM ì „ì²˜ë¦¬ í´ë˜ìŠ¤ (êµ¬ì¡° ì„¤ì •, ì‚¬ì „ ì„¤ì •, ì„ë² ë”©)"""

    def __init__(self, text_column='comment_text', num_topics=12):
        self.text_column = text_column
        self.num_topics = num_topics

        # ëª¨ë¸ êµ¬ì„±ìš”ì†Œ
        self.embedding_model = None
        self.classifier = None
        self.vectorizer = None
        self.vocab = None

        # ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
        self.preprocessing_results = {}

    def load_and_preprocess_data(self, csv_files):
        """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬"""
        all_texts = []
        file_info = []

        for csv_path in csv_files:
            try:
                print(f"ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘: {csv_path}")
                df = pd.read_csv(csv_path, encoding='utf-8')

                texts = df[self.text_column].dropna().drop_duplicates().tolist()
                texts = [text.strip() for text in texts if text.strip()]

                print(f"âœ… {len(texts):,}ê°œ ìœ íš¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ")

                for text in texts:
                    file_info.append({
                        'file_path': csv_path,
                        'text': text
                    })

                all_texts.extend(texts)

            except Exception as e:
                print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {csv_path}: {e}")
                continue

        print(f"ğŸ”„ ì´ {len(all_texts):,}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return all_texts, file_info

    def setup_hierarchical_structure(self):
        """ê³„ì¸µì  í† í”½ êµ¬ì¡° ì„¤ì •"""
        print("ğŸ—ï¸ ê³„ì¸µì  í† í”½ êµ¬ì¡° ì„¤ì • ì¤‘...")

        hierarchy = PoliticalDomainHyperParameters.POLITICAL_HIERARCHY
        
        print("ğŸ“Š ì •ì¹˜ ë„ë©”ì¸ ê³„ì¸µ êµ¬ì¡°:")
        print(f"   Level 1 (ì¸ë¬¼): {hierarchy['level1']}")
        print(f"   Level 2 (ì´ìŠˆ): {hierarchy['level2']}")
        print(f"   ì´ í† í”½ ìˆ˜: {len(hierarchy['level1']) * len(hierarchy['level2'])}")

        # í† í”½ ë§¤í•‘ ìƒì„±
        topic_mapping = {}
        topic_id = 0
        
        for person in hierarchy['level1']:
            for issue in hierarchy['level2']:
                topic_mapping[topic_id] = {
                    'person': person,
                    'issue': issue,
                    'label': f"{person}_{issue}"
                }
                topic_id += 1

        print("âœ… ê³„ì¸µì  êµ¬ì¡° ì„¤ì • ì™„ë£Œ")
        return hierarchy, topic_mapping

    def setup_political_dictionary(self):
        """ì •ì¹˜ì¸ë³„ ë‹¨ì–´ ì‚¬ì „ ì„¤ì •"""
        print("ğŸ“š ì •ì¹˜ì¸ë³„ ë‹¨ì–´ ì‚¬ì „ ì„¤ì • ì¤‘...")

        self.classifier = PoliticalDocumentClassifier()
        
        print("ğŸ‘¥ ì¸ë¬¼ë³„ í‚¤ì›Œë“œ:")
        for person, keywords in self.classifier.person_keywords.items():
            if keywords:  # neutralì€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì´ë¯€ë¡œ ì œì™¸
                print(f"   {person}: {keywords}")

        print("\nğŸ“‹ ì´ìŠˆë³„ í‚¤ì›Œë“œ:")
        for issue, keywords in self.classifier.issue_keywords.items():
            print(f"   {issue}: {keywords}")

        print("âœ… ì •ì¹˜ì¸ë³„ ë‹¨ì–´ ì‚¬ì „ ì„¤ì • ì™„ë£Œ")
        return self.classifier

    def classify_documents(self, texts):
        """ë¬¸ì„œ ë¶„ë¥˜ ìˆ˜í–‰"""
        print("ğŸ·ï¸ ë¬¸ì„œ ë¶„ë¥˜ ìˆ˜í–‰ ì¤‘...")
        
        if self.classifier is None:
            self.setup_political_dictionary()

        person_labels, issue_labels = self.classifier.classify_documents(texts)
        person_counts, issue_counts = self.classifier.get_classification_summary(person_labels, issue_labels)

        print(f"ğŸ“Š ì¸ë¬¼ë³„ ë¶„í¬: {person_counts}")
        print(f"ğŸ“Š ì´ìŠˆë³„ ë¶„í¬: {issue_counts}")

        return person_labels, issue_labels

    def create_bow_matrix(self, texts):
        """BoW í–‰ë ¬ ìƒì„±"""
        print("ğŸ“Š BoW í–‰ë ¬ ìƒì„± ì¤‘...")

        self.vectorizer = CountVectorizer(
            min_df=PoliticalDomainHyperParameters.MIN_DF,
            max_df=PoliticalDomainHyperParameters.MAX_DF,
            max_features=PoliticalDomainHyperParameters.MAX_FEATURES,
            token_pattern=r'\b[ê°€-í£]{2,}\b'
        )

        bow_matrix = self.vectorizer.fit_transform(texts).toarray()
        self.vocab = self.vectorizer.get_feature_names_out()

        print(f"ğŸ“Š ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")
        print(f"ğŸ“Š BoW í–‰ë ¬ í¬ê¸°: {bow_matrix.shape}")

        return bow_matrix

    def generate_embeddings(self, texts, person_labels, issue_labels):
        """ì„ë² ë”© ìƒì„±"""
        print("ğŸ”§ ì„ë² ë”© ìƒì„± ì¤‘...")

        # KoELECTRA ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedding_model = KoELECTRAEmbedding()

        # BoW í–‰ë ¬ ìƒì„±
        bow_matrix = self.create_bow_matrix(texts)

        # ë‹¨ì–´ ì„ë² ë”© ìƒì„±
        print("ğŸ“ ë‹¨ì–´ ì„ë² ë”© ìƒì„± ì¤‘...")
        word_embeddings = self.embedding_model.encode_words(self.vocab.tolist())

        # ì°¨ì› ì¡°ì •
        if word_embeddings.shape[1] != PoliticalDomainHyperParameters.EMBEDDING_DIM:
            target_dim = min(PoliticalDomainHyperParameters.EMBEDDING_DIM, word_embeddings.shape[1])
            if target_dim > 0:
                pca = PCA(n_components=target_dim)
                word_embeddings = pca.fit_transform(word_embeddings)
                word_embeddings = normalize(word_embeddings, norm='l2')

        # í† í”½ ì„ë² ë”© ì´ˆê¸°í™”
        print("ğŸ›ï¸ í† í”½ ì„ë² ë”© ì´ˆê¸°í™” ì¤‘...")
        topic_embeddings = self._initialize_political_topic_embeddings(
            texts, person_labels, issue_labels
        )

        print("âœ… ì„ë² ë”© ìƒì„± ì™„ë£Œ")
        return bow_matrix, word_embeddings, topic_embeddings

    def _initialize_political_topic_embeddings(self, texts, person_labels, issue_labels):
        """ì •ì¹˜ì¸ë³„ í† í”½ ì„ë² ë”© ì´ˆê¸°í™”"""
        print("ğŸ›ï¸ ì •ì¹˜ì¸ë³„ í† í”½ ì„ë² ë”© ì´ˆê¸°í™” ì¤‘...")

        doc_embeddings = self.embedding_model.encode_documents(texts)

        # ì •ì¹˜ì¸ë³„ í‰ê·  ì„ë² ë”©
        person_embeddings = {}
        hierarchy = PoliticalDomainHyperParameters.POLITICAL_HIERARCHY

        for person in hierarchy['level1']:
            if person == 'neutral':
                neutral_mask = np.array(person_labels) == 'neutral'
                if np.any(neutral_mask):
                    person_embeddings[person] = np.mean(doc_embeddings[neutral_mask], axis=0)
                else:
                    person_embeddings[person] = np.random.randn(doc_embeddings.shape[1])
            else:
                person_mask = np.array(person_labels) == person
                if np.any(person_mask):
                    person_embeddings[person] = np.mean(doc_embeddings[person_mask], axis=0)
                else:
                    person_embeddings[person] = np.random.randn(doc_embeddings.shape[1])

        # ì´ìŠˆë³„ í‰ê·  ì„ë² ë”©
        issue_embeddings = {}
        for issue in hierarchy['level2']:
            issue_mask = np.array(issue_labels) == issue
            if np.any(issue_mask):
                issue_embeddings[issue] = np.mean(doc_embeddings[issue_mask], axis=0)
            else:
                issue_embeddings[issue] = np.random.randn(doc_embeddings.shape[1])

        # ê³„ì¸µì  í† í”½ ì„ë² ë”© ìƒì„±
        topic_embeddings = []
        for person in hierarchy['level1']:
            for issue in hierarchy['level2']:
                topic_emb = (person_embeddings[person] * 0.6 +
                           issue_embeddings[issue] * 0.4)
                topic_embeddings.append(topic_emb)

        return np.array(topic_embeddings)

    def process_all(self, texts):
        """ëª¨ë“  ì „ì²˜ë¦¬ ê³¼ì •ì„ í•œ ë²ˆì— ì‹¤í–‰"""
        print("ğŸš€ === ETM ì „ì²˜ë¦¬ ê³¼ì • ì‹œì‘ ===")

        # 1. ê³„ì¸µì  êµ¬ì¡° ì„¤ì •
        hierarchy, topic_mapping = self.setup_hierarchical_structure()

        # 2. ì •ì¹˜ì¸ë³„ ì‚¬ì „ ì„¤ì •
        classifier = self.setup_political_dictionary()

        # 3. ë¬¸ì„œ ë¶„ë¥˜
        person_labels, issue_labels = self.classify_documents(texts)

        # 4. ì„ë² ë”© ìƒì„±
        bow_matrix, word_embeddings, topic_embeddings = self.generate_embeddings(
            texts, person_labels, issue_labels
        )

        # ê²°ê³¼ ì €ì¥
        self.preprocessing_results = {
            'hierarchy': hierarchy,
            'topic_mapping': topic_mapping,
            'classifier': classifier,
            'person_labels': person_labels,
            'issue_labels': issue_labels,
            'bow_matrix': bow_matrix,
            'word_embeddings': word_embeddings,
            'topic_embeddings': topic_embeddings,
            'vocab': self.vocab,
            'vectorizer': self.vectorizer
        }

        print("âœ… === ETM ì „ì²˜ë¦¬ ê³¼ì • ì™„ë£Œ ===")
        return self.preprocessing_results

def run_etm_preprocessing_pipeline():
    """ETM ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("ğŸš€ === ETM ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

    # CSV íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì •ì˜
    csv_files = [
        "comments_0204_0206.csv",
        "comments_0212_0214.csv",
        "comments_0222_0224.csv",
        "comments_0226_0228.csv",
        "comments_0303_0305.csv"
    ]

    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
    existing_files = [f for f in csv_files if os.path.exists(f)]
    print(f"ğŸ“ ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼: {len(existing_files)}ê°œ")

    if not existing_files:
        print("âŒ ì²˜ë¦¬í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ì „ì²˜ë¦¬ ê°ì²´ ìƒì„±
    preprocessor = ETMPreprocessor(text_column='comment_text', num_topics=12)

    # 1. ë°ì´í„° ë¡œë“œ
    all_texts, file_info = preprocessor.load_and_preprocess_data(existing_files)

    if not all_texts:
        print("âŒ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ì „ì²´ ì „ì²˜ë¦¬ ê³¼ì • ì‹¤í–‰
    results = preprocessor.process_all(all_texts)

    print("\nğŸ‰ === ETM ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
    print(f"ğŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"   ë¬¸ì„œ ìˆ˜: {results['bow_matrix'].shape[0]:,}")
    print(f"   ì–´íœ˜ ìˆ˜: {results['bow_matrix'].shape[1]:,}")
    print(f"   í† í”½ ìˆ˜: {results['topic_embeddings'].shape[0]}")
    print(f"   ì„ë² ë”© ì°¨ì›: {results['word_embeddings'].shape[1]}")

    return preprocessor, results

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    preprocessor, results = run_etm_preprocessing_pipeline()

