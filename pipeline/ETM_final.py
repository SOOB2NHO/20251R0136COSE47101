import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.optim import Adam
from transformers import AutoTokenizer, AutoModel
from sklearn.preprocessing import normalize
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
from sklearn.feature_extraction.text import CountVectorizer
from gensim import corpora, models
from gensim.models import CoherenceModel
from tqdm import tqdm
import os

# GPU í™˜ê²½ ì„¤ì •
device = "cuda" if torch.cuda.is_available() else "cpu"
print(f"ì‚¬ìš© ì¤‘ì¸ ë””ë°”ì´ìŠ¤: {device}")

class PoliticalDomainHyperParameters:
    """ì •ì¹˜ ë„ë©”ì¸ íŠ¹í™” í•˜ì´í¼íŒŒë¼ë¯¸í„°"""

    # ëª¨ë¸ ì„¤ì •
    MODEL_NAME = 'monologg/koelectra-base-v3-discriminator'
    MAX_LENGTH = 128
    BATCH_SIZE = 32

    # ê³„ì¸µì  í† í”½ êµ¬ì¡°
    POLITICAL_HIERARCHY = {
        'level1': ['yoon', 'lee', 'neutral'],
        'level2': ['policy', 'scandal', 'campaign', 'general'],
        'level3_topics_per_person': 3
    }

    # ETM íŒŒë¼ë¯¸í„°
    NUM_TOPICS_TOTAL = 12
    EMBEDDING_DIM = 256
    HIDDEN_DIM = 400
    DROPOUT = 0.5

    # í•™ìŠµ íŒŒë¼ë¯¸í„°
    EPOCHS = 100
    LEARNING_RATE = 0.001
    BETA1 = 0.99
    BETA2 = 0.999

    # ì •ì¹˜ì  ëŒ€ë¦½ ì •ê·œí™” íŒŒë¼ë¯¸í„°
    OPPOSITION_WEIGHT = 2.0

    # ì–´íœ˜ ì„¤ì •
    MIN_DF = 3
    MAX_DF = 0.8
    MAX_FEATURES = 1000

class KoELECTRAEmbedding:
    """KoELECTRA ì„ë² ë”© ëª¨ë¸"""

    def __init__(self, model_name=PoliticalDomainHyperParameters.MODEL_NAME):
        print(f"KoELECTRA ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModel.from_pretrained(model_name)

        if torch.cuda.is_available():
            self.model = self.model.to(device)

        print("KoELECTRA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

    def encode_documents(self, texts, batch_size=PoliticalDomainHyperParameters.BATCH_SIZE):
        """ë¬¸ì„œ ì„ë² ë”© ìƒì„±"""
        embeddings = []
        self.model.eval()

        with torch.no_grad():
            for i in tqdm(range(0, len(texts), batch_size), desc="ë¬¸ì„œ ì„ë² ë”© ìƒì„±"):
                batch = texts[i:i+batch_size]
                inputs = self.tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=PoliticalDomainHyperParameters.MAX_LENGTH,
                    return_tensors='pt'
                )

                if torch.cuda.is_available():
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                outputs = self.model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_embeddings)

        return np.vstack(embeddings)

    def encode_words(self, words, batch_size=PoliticalDomainHyperParameters.BATCH_SIZE):
        """ë‹¨ì–´ ì„ë² ë”© ìƒì„±"""
        embeddings = []
        self.model.eval()

        with torch.no_grad():
            for i in tqdm(range(0, len(words), batch_size), desc="ë‹¨ì–´ ì„ë² ë”© ìƒì„±"):
                batch = words[i:i+batch_size]
                inputs = self.tokenizer(
                    batch,
                    padding=True,
                    truncation=True,
                    max_length=32,
                    return_tensors='pt'
                )

                if torch.cuda.is_available():
                    inputs = {k: v.to(device) for k, v in inputs.items()}

                outputs = self.model(**inputs)
                batch_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()
                embeddings.append(batch_embeddings)

        return np.vstack(embeddings)

class PoliticalDocumentClassifier:
    """ì •ì¹˜ì¸ë³„ ë¬¸ì„œ ë¶„ë¥˜ê¸°"""

    def __init__(self):
        self.person_keywords = {
            'yoon': ['ìœ¤ì„ì—´', 'ìœ¤', 'ëŒ€í†µë ¹', 'í˜„ì •ë¶€', 'ì •ë¶€', 'ì—¬ë‹¹', 'êµ­ë¯¼ì˜í˜'],
            'lee': ['ì´ì¬ëª…', 'ì´', 'ë¯¼ì£¼ë‹¹', 'ì•¼ë‹¹', 'ë”ë¶ˆì–´ë¯¼ì£¼ë‹¹'],
            'neutral': []
        }

        self.issue_keywords = {
            'policy': ['ì •ì±…', 'ê³µì•½', 'ë²•ì•ˆ', 'ì œë„', 'ê°œí˜', 'ì˜ˆì‚°', 'ê²½ì œ', 'ë³µì§€'],
            'scandal': ['ìˆ˜ì‚¬', 'ê²€ì°°', 'ì¬íŒ', 'í˜ì˜', 'ë¹„ë¦¬', 'ë…¼ë€', 'ì˜í˜¹'],
            'campaign': ['ì„ ê±°', 'ìº í˜ì¸', 'ìœ ì„¸', 'ê³µì²œ', 'í›„ë³´', 'ì§€ì§€ìœ¨'],
            'general': ['ë°œì–¸', 'íšŒê²¬', 'ì„±ëª…', 'ì…ì¥', 'ë°˜ì‘', 'ë¹„íŒ']
        }

    def classify_documents(self, texts):
        """ë¬¸ì„œë¥¼ ì¸ë¬¼ë³„, ì´ìŠˆë³„ë¡œ ë¶„ë¥˜"""
        person_labels = []
        issue_labels = []

        for text in tqdm(texts, desc="ë¬¸ì„œ ë¶„ë¥˜ ì¤‘"):
            text_lower = text.lower()

            # ì¸ë¬¼ ë¶„ë¥˜
            person_scores = {}
            for person, keywords in self.person_keywords.items():
                if person == 'neutral':
                    continue
                score = sum(text_lower.count(keyword) for keyword in keywords)
                person_scores[person] = score

            if max(person_scores.values()) > 0:
                dominant_person = max(person_scores, key=person_scores.get)
            else:
                dominant_person = 'neutral'

            person_labels.append(dominant_person)

            # ì´ìŠˆ ë¶„ë¥˜
            issue_scores = {}
            for issue, keywords in self.issue_keywords.items():
                score = sum(text_lower.count(keyword) for keyword in keywords)
                issue_scores[issue] = score

            if max(issue_scores.values()) > 0:
                dominant_issue = max(issue_scores, key=issue_scores.get)
            else:
                dominant_issue = 'general'

            issue_labels.append(dominant_issue)

        return person_labels, issue_labels

class HierarchicalPoliticalETM(nn.Module):
    """ê³„ì¸µì  ì •ì¹˜ ë„ë©”ì¸ íŠ¹í™” ETM"""

    def __init__(self, vocab_size, num_topics, embedding_dim, hidden_dim,
                 word_embeddings=None, topic_embeddings=None, dropout=0.5):
        super(HierarchicalPoliticalETM, self).__init__()

        self.vocab_size = vocab_size
        self.num_topics = num_topics
        self.embedding_dim = embedding_dim
        self.hidden_dim = hidden_dim

        # ë³€ë¶„ ì¸ì½”ë”
        self.encoder_layers = nn.Sequential(
            nn.Linear(vocab_size, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout),
        )

        self.mu_layer = nn.Linear(hidden_dim, num_topics)
        self.logvar_layer = nn.Linear(hidden_dim, num_topics)

        # ë‹¨ì–´ ì„ë² ë”©
        if word_embeddings is not None:
            if word_embeddings.shape[1] != embedding_dim:
                self.word_embedding_transform = nn.Linear(word_embeddings.shape[1], embedding_dim, bias=False)
                self.word_embeddings_raw = nn.Parameter(torch.FloatTensor(word_embeddings))
                self.word_embeddings_raw.requires_grad = False
            else:
                self.word_embeddings = nn.Parameter(torch.FloatTensor(word_embeddings))
                self.word_embeddings.requires_grad = False
                self.word_embedding_transform = None
        else:
            self.word_embeddings = nn.Parameter(torch.randn(vocab_size, embedding_dim))
            self.word_embedding_transform = None

        # í† í”½ ì„ë² ë”©
        if topic_embeddings is not None:
            if topic_embeddings.shape[1] != embedding_dim:
                self.topic_embedding_transform = nn.Linear(topic_embeddings.shape[1], embedding_dim, bias=False)
                self.topic_embeddings_raw = nn.Parameter(torch.FloatTensor(topic_embeddings))
            else:
                self.topic_embeddings = nn.Parameter(torch.FloatTensor(topic_embeddings))
                self.topic_embedding_transform = None
        else:
            self.topic_embeddings = nn.Parameter(torch.randn(num_topics, embedding_dim))
            self.topic_embedding_transform = None

        # ë°°ì¹˜ ì •ê·œí™”
        self.batch_norm = nn.BatchNorm1d(num_topics, affine=False)

    def get_word_embeddings(self):
        """ë‹¨ì–´ ì„ë² ë”© ë°˜í™˜"""
        if self.word_embedding_transform is not None:
            return self.word_embedding_transform(self.word_embeddings_raw)
        else:
            return self.word_embeddings

    def get_topic_embeddings(self):
        """í† í”½ ì„ë² ë”© ë°˜í™˜"""
        if self.topic_embedding_transform is not None:
            return self.topic_embedding_transform(self.topic_embeddings_raw)
        else:
            return self.topic_embeddings

    def get_beta(self):
        """í† í”½-ë‹¨ì–´ ë¶„í¬ ê³„ì‚°"""
        word_embeddings = self.get_word_embeddings()
        topic_embeddings = self.get_topic_embeddings()

        # êµ¬ë©´ ê³µê°„ì—ì„œì˜ ë°©í–¥ì„± ìœ ì‚¬ë„
        word_embeddings_norm = F.normalize(word_embeddings, p=2, dim=1)
        topic_embeddings_norm = F.normalize(topic_embeddings, p=2, dim=1)

        logits = torch.mm(topic_embeddings_norm, word_embeddings_norm.T)
        beta = F.softmax(logits, dim=1)

        return beta

    def forward(self, bow):
        """ìˆœì „íŒŒ"""
        # ì •ê·œí™”ëœ BoW
        normalized_bow = bow / (bow.sum(1, keepdim=True) + 1e-8)

        # ë³€ë¶„ ì¶”ë¡ 
        h = self.encoder_layers(normalized_bow)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)

        # ì¬ë§¤ê°œí™”
        if self.training:
            std = torch.exp(0.5 * logvar)
            eps = torch.randn_like(std)
            theta = mu + eps * std
        else:
            theta = mu

        # ë°°ì¹˜ ì •ê·œí™”
        if theta.size(0) > 1:
            theta = self.batch_norm(theta)

        # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í† í”½ ë¶„í¬ ìƒì„±
        theta = F.softmax(theta, dim=1)

        # í† í”½-ë‹¨ì–´ ë¶„í¬
        beta = self.get_beta()

        # ë¬¸ì„œ-ë‹¨ì–´ ë¶„í¬ ì¬êµ¬ì„±
        word_dist = torch.mm(theta, beta)

        return word_dist, theta, mu, logvar, beta

    def get_topics(self, vocab, top_k=10):
        """í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ"""
        beta = self.get_beta()
        topics = []

        for k in range(self.num_topics):
            topic_words = []
            _, top_indices = torch.topk(beta[k], top_k)
            for idx in top_indices:
                word = vocab[idx.item()]
                prob = beta[k][idx].item()
                topic_words.append((word, prob))
            topics.append(topic_words)

        return topics

class EnhancedHierarchicalETM:
    """í–¥ìƒëœ ê³„ì¸µì  ETM í´ë˜ìŠ¤"""

    def __init__(self, text_column='comment_text', num_topics=12):
        self.text_column = text_column
        self.num_topics = num_topics

        # ëª¨ë¸ êµ¬ì„±ìš”ì†Œ
        self.embedding_model = None
        self.classifier = None
        self.etm_model = None
        self.vectorizer = None
        self.vocab = None

        # ì„±ëŠ¥ í‰ê°€ ê²°ê³¼
        self.evaluation_results = {}

    def load_and_preprocess_data(self, csv_files):
        """ì—¬ëŸ¬ CSV íŒŒì¼ì„ ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬"""
        all_texts = []
        file_info = []

        for csv_path in csv_files:
            try:
                print(f"ğŸ“ ë°ì´í„° ë¡œë“œ ì¤‘: {csv_path}")
                df = pd.read_csv(csv_path, encoding='utf-8')

                texts = df[self.text_column].dropna().drop_duplicates().tolist()
                texts = [text.strip() for text in texts if text.strip()]

                print(f"âœ… {len(texts):,}ê°œ ìœ íš¨ í…ìŠ¤íŠ¸ ì¶”ì¶œ")

                for text in texts:
                    file_info.append({
                        'file_path': csv_path,
                        'text': text
                    })

                all_texts.extend(texts)

            except Exception as e:
                print(f"âŒ íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {csv_path}: {e}")
                continue

        print(f"ğŸ”„ ì´ {len(all_texts):,}ê°œ ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ")
        return all_texts, file_info

    def initialize_embeddings_and_topics(self, texts, person_labels, issue_labels):
        """ì„ë² ë”© ë° í† í”½ ì´ˆê¸°í™”"""
        print("ğŸ”§ ì„ë² ë”© ë° í† í”½ ì´ˆê¸°í™” ì¤‘...")

        # KoELECTRA ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        self.embedding_model = KoELECTRAEmbedding()

        # BoW í–‰ë ¬ ìƒì„±
        self.vectorizer = CountVectorizer(
            min_df=PoliticalDomainHyperParameters.MIN_DF,
            max_df=PoliticalDomainHyperParameters.MAX_DF,
            max_features=PoliticalDomainHyperParameters.MAX_FEATURES,
            token_pattern=r'\b[ê°€-í£]{2,}\b'
        )

        bow_matrix = self.vectorizer.fit_transform(texts).toarray()
        self.vocab = self.vectorizer.get_feature_names_out()

        print(f"ğŸ“Š ì–´íœ˜ í¬ê¸°: {len(self.vocab)}")

        # ë‹¨ì–´ ì„ë² ë”© ìƒì„±
        word_embeddings = self.embedding_model.encode_words(self.vocab.tolist())

        # ì°¨ì› ì¡°ì •
        if word_embeddings.shape[1] != PoliticalDomainHyperParameters.EMBEDDING_DIM:
            target_dim = min(PoliticalDomainHyperParameters.EMBEDDING_DIM, word_embeddings.shape[1])
            if target_dim > 0:
                pca = PCA(n_components=target_dim)
                word_embeddings = pca.fit_transform(word_embeddings)
                word_embeddings = normalize(word_embeddings, norm='l2')

        # í† í”½ ì„ë² ë”© ì´ˆê¸°í™”
        topic_embeddings = self._initialize_political_topic_embeddings(
            texts, person_labels, issue_labels
        )

        return bow_matrix, word_embeddings, topic_embeddings

    def _initialize_political_topic_embeddings(self, texts, person_labels, issue_labels):
        """ì •ì¹˜ì¸ë³„ í† í”½ ì„ë² ë”© ì´ˆê¸°í™”"""
        print("ğŸ›ï¸ ì •ì¹˜ì¸ë³„ í† í”½ ì„ë² ë”© ì´ˆê¸°í™” ì¤‘...")

        doc_embeddings = self.embedding_model.encode_documents(texts)

        # ì •ì¹˜ì¸ë³„ í‰ê·  ì„ë² ë”©
        person_embeddings = {}
        hierarchy = PoliticalDomainHyperParameters.POLITICAL_HIERARCHY

        for person in hierarchy['level1']:
            if person == 'neutral':
                neutral_mask = np.array(person_labels) == 'neutral'
                if np.any(neutral_mask):
                    person_embeddings[person] = np.mean(doc_embeddings[neutral_mask], axis=0)
                else:
                    person_embeddings[person] = np.random.randn(doc_embeddings.shape[1])
            else:
                person_mask = np.array(person_labels) == person
                if np.any(person_mask):
                    person_embeddings[person] = np.mean(doc_embeddings[person_mask], axis=0)
                else:
                    person_embeddings[person] = np.random.randn(doc_embeddings.shape[1])

        # ì´ìŠˆë³„ í‰ê·  ì„ë² ë”©
        issue_embeddings = {}
        for issue in hierarchy['level2']:
            issue_mask = np.array(issue_labels) == issue
            if np.any(issue_mask):
                issue_embeddings[issue] = np.mean(doc_embeddings[issue_mask], axis=0)
            else:
                issue_embeddings[issue] = np.random.randn(doc_embeddings.shape[1])

        # ê³„ì¸µì  í† í”½ ì„ë² ë”© ìƒì„±
        topic_embeddings = []
        for person in hierarchy['level1']:
            for issue in hierarchy['level2']:
                topic_emb = (person_embeddings[person] * 0.6 +
                           issue_embeddings[issue] * 0.4)
                topic_embeddings.append(topic_emb)

        return np.array(topic_embeddings)

    def train_etm_model(self, bow_matrix, word_embeddings, topic_embeddings):
        """ETM ëª¨ë¸ í•™ìŠµ"""
        print("ğŸš€ ê³„ì¸µì  ETM ëª¨ë¸ í•™ìŠµ ì¤‘...")

        # ëª¨ë¸ ìƒì„±
        self.etm_model = HierarchicalPoliticalETM(
            vocab_size=len(self.vocab),
            num_topics=self.num_topics,
            embedding_dim=word_embeddings.shape[1],
            hidden_dim=PoliticalDomainHyperParameters.HIDDEN_DIM,
            word_embeddings=word_embeddings,
            topic_embeddings=topic_embeddings,
            dropout=PoliticalDomainHyperParameters.DROPOUT
        ).to(device)

        # ì˜µí‹°ë§ˆì´ì €
        optimizer = Adam(
            self.etm_model.parameters(),
            lr=PoliticalDomainHyperParameters.LEARNING_RATE,
            betas=(PoliticalDomainHyperParameters.BETA1, PoliticalDomainHyperParameters.BETA2)
        )

        # ë°ì´í„° ë¡œë”
        dataset = torch.utils.data.TensorDataset(torch.FloatTensor(bow_matrix))
        data_loader = torch.utils.data.DataLoader(
            dataset,
            batch_size=PoliticalDomainHyperParameters.BATCH_SIZE,
            shuffle=True,
            drop_last=False
        )

        # í•™ìŠµ
        for epoch in range(PoliticalDomainHyperParameters.EPOCHS):
            self.etm_model.train()
            total_loss = 0

            for batch in data_loader:
                bow = batch[0].to(device)
                optimizer.zero_grad()

                # ìˆœì „íŒŒ
                word_dist, theta, mu, logvar, beta = self.etm_model(bow)

                # ì†ì‹¤ ê³„ì‚°
                recon_loss = -torch.sum(bow * torch.log(word_dist + 1e-8), dim=1)
                kl_div = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1)
                loss = torch.mean(recon_loss + kl_div)

                # ì—­ì „íŒŒ
                loss.backward()
                optimizer.step()

                total_loss += loss.item()

            if (epoch + 1) % 20 == 0:
                print(f"Epoch {epoch+1}/{PoliticalDomainHyperParameters.EPOCHS}, Loss: {total_loss/len(data_loader):.4f}")

        print("âœ… ETM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")

    def calculate_coherence_score(self, texts):
        """Coherence ì ìˆ˜ ê³„ì‚°"""
        print("ğŸ“ˆ Coherence ì ìˆ˜ ê³„ì‚° ì¤‘...")

        try:
            # í† í°í™”ëœ í…ìŠ¤íŠ¸ ìƒì„±
            tokenized_texts = []
            for text in texts:
                tokens = [word for word in text.split() if len(word) >= 2]
                tokenized_texts.append(tokens)

            # Gensim ì‚¬ì „ ë° ì½”í¼ìŠ¤ ìƒì„±
            dictionary = corpora.Dictionary(tokenized_texts)
            dictionary.filter_extremes(no_below=2, no_above=0.8, keep_n=1000)
            corpus = [dictionary.doc2bow(text) for text in tokenized_texts]

            # í† í”½ë³„ ìƒìœ„ ë‹¨ì–´ ì¶”ì¶œ
            topics = self.etm_model.get_topics(self.vocab, top_k=10)
            topic_words = [[word for word, _ in topic] for topic in topics]

            # Coherence ëª¨ë¸ ìƒì„± ë° ì ìˆ˜ ê³„ì‚°
            coherence_model = CoherenceModel(
                topics=topic_words,
                texts=tokenized_texts,
                dictionary=dictionary,
                coherence='c_v'
            )
            coherence_score = coherence_model.get_coherence()

            print(f"ğŸ¯ Coherence Score: {coherence_score:.4f}")
            return coherence_score

        except Exception as e:
            print(f"âŒ Coherence ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def calculate_topic_diversity(self, top_k=25):
        """í† í”½ ë‹¤ì–‘ì„± ê³„ì‚°"""
        print("ğŸ“Š Topic Diversity ê³„ì‚° ì¤‘...")

        try:
            topics = self.etm_model.get_topics(self.vocab, top_k=top_k)

            # ê° í† í”½ì˜ ìƒìœ„ kê°œ ë‹¨ì–´ ì§‘í•©
            topic_words = []
            for topic in topics:
                words = set([word for word, _ in topic])
                topic_words.append(words)

            # í† í”½ ê°„ ê³ ìœ  ë‹¨ì–´ ë¹„ìœ¨ ê³„ì‚°
            unique_words = set()
            total_words = 0

            for words in topic_words:
                unique_words.update(words)
                total_words += len(words)

            diversity_score = len(unique_words) / total_words if total_words > 0 else 0

            print(f"ğŸŒˆ Topic Diversity: {diversity_score:.4f}")
            return diversity_score

        except Exception as e:
            print(f"âŒ Topic Diversity ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    def calculate_perplexity(self, bow_matrix):
        """Perplexity ì ìˆ˜ ê³„ì‚°"""
        print("ğŸ” Perplexity ì ìˆ˜ ê³„ì‚° ì¤‘...")

        try:
            self.etm_model.eval()
            total_log_likelihood = 0
            total_words = 0

            with torch.no_grad():
                bow_tensor = torch.FloatTensor(bow_matrix).to(device)
                word_dist, _, _, _, _ = self.etm_model(bow_tensor)

                # ë¡œê·¸ ìš°ë„ ê³„ì‚°
                log_likelihood = torch.sum(bow_tensor * torch.log(word_dist + 1e-8))
                total_log_likelihood += log_likelihood.item()
                total_words += torch.sum(bow_tensor).item()

            # Perplexity ê³„ì‚°
            perplexity = np.exp(-total_log_likelihood / total_words)

            print(f"ğŸ“‰ Perplexity: {perplexity:.4f}")
            return perplexity

        except Exception as e:
            print(f"âŒ Perplexity ê³„ì‚° ì‹¤íŒ¨: {e}")
            return float('inf')

    def evaluate_model_performance(self, texts, bow_matrix):
        """ì¢…í•©ì ì¸ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€"""
        print("\nğŸ¯ === ê³„ì¸µì  ETM ëª¨ë¸ ì„±ëŠ¥ í‰ê°€ ===")

        # Coherence ì ìˆ˜
        coherence_score = self.calculate_coherence_score(texts)

        # Topic Diversity
        diversity_score = self.calculate_topic_diversity()

        # Perplexity
        perplexity_score = self.calculate_perplexity(bow_matrix)

        # ê²°ê³¼ ì €ì¥
        self.evaluation_results = {
            'coherence_score': coherence_score,
            'topic_diversity': diversity_score,
            'perplexity': perplexity_score,
            'num_topics': self.num_topics
        }

        print(f"\nğŸ“Š === ì„±ëŠ¥ í‰ê°€ ìš”ì•½ ===")
        print(f"Coherence Score: {coherence_score:.4f}")
        print(f"Topic Diversity: {diversity_score:.4f}")
        print(f"Perplexity: {perplexity_score:.4f}")

        return self.evaluation_results

    def display_topics(self, num_words=10):
        """í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ì¶œë ¥"""
        print("\nğŸ“‹ === ê³„ì¸µì  í† í”½ë³„ ì£¼ìš” ë‹¨ì–´ ===")

        topics = self.etm_model.get_topics(self.vocab, top_k=num_words)
        topics_info = []

        for topic_idx, topic_words in enumerate(topics):
            print(f"\nğŸ·ï¸  í† í”½ {topic_idx + 1}:")
            for word, weight in topic_words:
                print(f"   {word}: {weight:.4f}")

            topics_info.append({
                'topic_id': topic_idx + 1,
                'words': [word for word, _ in topic_words],
                'weights': [weight for _, weight in topic_words]
            })

        return topics_info

    def get_document_topics(self, bow_matrix, texts):
        """ê° ë¬¸ì„œì˜ í† í”½ ë¶„í¬ ê³„ì‚°"""
        print("ğŸ“„ ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ê³„ì‚° ì¤‘...")

        self.etm_model.eval()
        document_topics = []

        with torch.no_grad():
            bow_tensor = torch.FloatTensor(bow_matrix).to(device)
            _, theta, _, _, _ = self.etm_model(bow_tensor)

            for i, (text, doc_topics) in enumerate(zip(texts, theta.cpu().numpy())):
                dominant_topic = doc_topics.argmax()
                dominant_prob = doc_topics.max()

                document_topics.append({
                    'document_id': i,
                    'text': text,
                    'dominant_topic': dominant_topic + 1,
                    'probability': dominant_prob,
                    'all_topic_probs': doc_topics.tolist()
                })

        return pd.DataFrame(document_topics)

def run_enhanced_hierarchical_etm_pipeline():
    """í–¥ìƒëœ ê³„ì¸µì  ETM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    print("ğŸš€ === í–¥ìƒëœ ê³„ì¸µì  ETM í† í”½ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")

    # CSV íŒŒì¼ ë¦¬ìŠ¤íŠ¸ ì •ì˜
    csv_files = [
        "comments_0204_0206.csv",
        "comments_0212_0214.csv",
        "comments_0222_0224.csv",
        "comments_0226_0228.csv",
        "comments_0303_0305.csv"
    ]

    # ì¡´ì¬í•˜ëŠ” íŒŒì¼ë§Œ í•„í„°ë§
    existing_files = [f for f in csv_files if os.path.exists(f)]
    print(f"ğŸ“ ì²˜ë¦¬ ê°€ëŠ¥í•œ íŒŒì¼: {len(existing_files)}ê°œ")

    if not existing_files:
        print("âŒ ì²˜ë¦¬í•  CSV íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # ê³„ì¸µì  ETM ëª¨ë¸ë§ ê°ì²´ ìƒì„±
    etm_analyzer = EnhancedHierarchicalETM(
        text_column='comment_text',
        num_topics=12
    )

    # 1. ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
    all_texts, file_info = etm_analyzer.load_and_preprocess_data(existing_files)

    if not all_texts:
        print("âŒ ìœ íš¨í•œ í…ìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2. ì •ì¹˜ì¸ë³„ ë¬¸ì„œ ë¶„ë¥˜
    classifier = PoliticalDocumentClassifier()
    person_labels, issue_labels = classifier.classify_documents(all_texts)

    print(f"ğŸ“Š ì¸ë¬¼ë³„ ë¶„í¬: {dict(zip(*np.unique(person_labels, return_counts=True)))}")
    print(f"ğŸ“Š ì´ìŠˆë³„ ë¶„í¬: {dict(zip(*np.unique(issue_labels, return_counts=True)))}")

    # 3. ì„ë² ë”© ë° í† í”½ ì´ˆê¸°í™”
    bow_matrix, word_embeddings, topic_embeddings = etm_analyzer.initialize_embeddings_and_topics(
        all_texts, person_labels, issue_labels
    )

    # 4. ETM ëª¨ë¸ í•™ìŠµ
    etm_analyzer.train_etm_model(bow_matrix, word_embeddings, topic_embeddings)

    # 5. ì„±ëŠ¥ í‰ê°€
    evaluation_results = etm_analyzer.evaluate_model_performance(all_texts, bow_matrix)

    # 6. í† í”½ ë¶„ì„ ë° ê²°ê³¼ ì¶œë ¥
    topics_info = etm_analyzer.display_topics(num_words=10)

    # 7. ë¬¸ì„œë³„ í† í”½ ë¶„í¬ ê³„ì‚°
    document_topics_df = etm_analyzer.get_document_topics(bow_matrix, all_texts)

    print("\nğŸ‰ === ê³„ì¸µì  ETM í† í”½ ëª¨ë¸ë§ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
    print(f"ğŸ“Š ìµœì¢… ì„±ëŠ¥ ì§€í‘œ:")
    print(f"   Coherence: {evaluation_results['coherence_score']:.4f}")
    print(f"   Diversity: {evaluation_results['topic_diversity']:.4f}")
    print(f"   Perplexity: {evaluation_results['perplexity']:.4f}")

    return etm_analyzer, evaluation_results

# ì‹¤í–‰ ì½”ë“œ
if __name__ == "__main__":
    # í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜ í™•ì¸
    try:
        import gensim
        print("âœ… Gensim ë¼ì´ë¸ŒëŸ¬ë¦¬ í™•ì¸")
    except ImportError:
        print("âŒ Gensim ì„¤ì¹˜ í•„ìš”")
        exit()

    # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    etm_model, results = run_enhanced_hierarchical_etm_pipeline()
