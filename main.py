# # main.py - ë„¤ì´ë²„ APIë¡œ ë‰´ìŠ¤ URL ìˆ˜ì§‘ í›„ ëŒ“ê¸€ ìˆ˜ì§‘ (Selenium ì—†ì´ API ë°©ì‹)

# import pandas as pd
# import requests
# import re
# import time

# # âœ… ë„¤ì´ë²„ API ì¸ì¦ ì •ë³´
# CLIENT_ID = "DkqIEaI_ltBe70Xdm4W6"  # ë³¸ì¸ì˜ API í‚¤ ì…ë ¥
# CLIENT_SECRET = "BAA_gdcR17"
# HEADERS = {
#     "X-Naver-Client-Id": CLIENT_ID,
#     "X-Naver-Client-Secret": CLIENT_SECRET
# }
# NAVER_NEWS_API = "https://openapi.naver.com/v1/search/news.json"

# # âœ… ê¸°ì‚¬ URL ìˆ˜ì§‘ í•¨ìˆ˜
# def get_news_urls(query="ì´ì¬ëª…|ê¹€ë¬¸ìˆ˜|ëŒ€ì„ ", max_articles=2000):
#     collected_data = []
#     for start in range(1, max_articles + 1, 100):
#         display = min(100, max_articles - len(collected_data))
#         params = {
#             "query": query,
#             "display": display,
#             "start": start,
#             "sort": "date"
#         }
#         res = requests.get(NAVER_NEWS_API, headers=HEADERS, params=params)
#         data = res.json()
#         for item in data.get("items", []):
#             title = re.sub(r"<.*?>", "", item["title"])
#             link = item["link"]
#             pub_date = item["pubDate"]
#             if "n.news.naver.com" in link:
#                 collected_data.append([title, link, pub_date])
#         if len(data.get("items", [])) < 100:
#             break
#         time.sleep(0.5)
#     return collected_data

# # âœ… ìˆ˜ì§‘ëœ ê¸°ì‚¬ ì €ì¥ ë° ëŒ“ê¸€ ìˆ˜ì§‘ ì‹œì‘
# news_output_csv = "filtered_naver_news.csv"
# news_data = get_news_urls(query="21ëŒ€ ëŒ€ì„ ", max_articles=2000)
# df_news = pd.DataFrame(news_data, columns=["ì œëª©", "ê¸°ì‚¬URL", "ì‘ì„±ì¼"])
# df_news.to_csv(news_output_csv, index=False, encoding='utf-8-sig')
# print(f"ğŸ“° ê¸°ì‚¬ URL {len(df_news)}ê°œ ì €ì¥ ì™„ë£Œ: {news_output_csv}")

# # âœ… ëŒ“ê¸€ ìˆ˜ì§‘ ëŒ€ìƒ íŒŒì¼ ê²½ë¡œ
# csv_path = news_output_csv
# output_path = "naver_comments_result.csv"

# # âœ… CSV ë¡œë“œ ë° URL ì»¬ëŸ¼ ì„ íƒ
# df = pd.read_csv(csv_path)
# if 'Naverlink' in df.columns:
#     urls = df['Naverlink']
# elif 'ê¸°ì‚¬URL' in df.columns:
#     urls = df['ê¸°ì‚¬URL']
# else:
#     raise ValueError("CSV íŒŒì¼ì— 'Naverlink' ë˜ëŠ” 'ê¸°ì‚¬URL' ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤.")

# # âœ… ëŒ“ê¸€ ë¦¬ìŠ¤íŠ¸ í‰íƒ„í™” í•¨ìˆ˜
# def flatten(l):
#     return [item for sublist in l for item in (sublist if isinstance(sublist, list) else [sublist])]

# # âœ… ì „ì²´ ëŒ“ê¸€ ì €ì¥ ë¦¬ìŠ¤íŠ¸ ì´ˆê¸°í™”
# total_comments = []

# # âœ… ê° ê¸°ì‚¬ë³„ ëŒ“ê¸€ ìˆ˜ì§‘ ë°˜ë³µ
# for idx, url in enumerate(urls):
#     try:
#         print(f"{idx+1}/{len(urls)}ë²ˆì§¸ ê¸°ì‚¬ ëŒ“ê¸€ ìˆ˜ì§‘ ì¤‘: {url}")

#         oid = url.split("article/")[1].split("/")[0]
#         aid = url.split("article/")[1].split("/")[1].split("?")[0]

#         page = 1
#         all_comments = []

#         header = {
#             "User-agent": "Mozilla/5.0",
#             "referer": url
#         }

#         while True:
#             callback_id = f"jQuery1123{int(time.time() * 1000)}"
#             c_url = (
#                 f"https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json"
#                 f"?ticket=news&templateId=default_society&pool=cbox5&_callback={callback_id}"
#                 f"&lang=ko&country=KR&objectId=news{oid}%2C{aid}&pageSize=20&indexSize=10"
#                 f"&listType=OBJECT&pageType=more&page={page}&sort=FAVORITE"
#             )

#             res = requests.get(c_url, headers=header)
#             html = res.text

#             if 'comment":' not in html:
#                 print("ğŸš« ëŒ“ê¸€ ì—†ìŒ ë˜ëŠ” JSON êµ¬ì¡° ì´ìƒ")
#                 break

#             try:
#                 total_comm = int(html.split('comment":')[1].split(",")[0])
#             except Exception as e:
#                 print("ğŸš« ëŒ“ê¸€ ìˆ˜ íŒŒì‹± ì˜¤ë¥˜:", e)
#                 break

#             matches = re.findall(r'"contents":"(.*?)","userIdNo"', html)
#             if not matches:
#                 break

#             all_comments.extend(matches)

#             if page * 20 >= total_comm:
#                 break
#             page += 1
#             time.sleep(0.1)

#         total_comments.append({
#             "url": url,
#             "ëŒ“ê¸€ ìˆ˜": len(all_comments),
#             "ëŒ“ê¸€": flatten(all_comments)
#         })

#     except Exception as e:
#         print(f"â›” ì˜¤ë¥˜ ë°œìƒ: {e}")
#         total_comments.append({
#             "url": url,
#             "ëŒ“ê¸€ ìˆ˜": 0,
#             "ëŒ“ê¸€": []
#         })

# # âœ… ê²°ê³¼ë¥¼ CSVë¡œ ì €ì¥
# df_result = pd.DataFrame(total_comments)
# df_result.to_csv(output_path, index=False, encoding='utf-8-sig')
# print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

# import requests
# from bs4 import BeautifulSoup
# from selenium import webdriver
# from selenium.webdriver.chrome.options import Options
# from openpyxl import Workbook, load_workbook
# from datetime import datetime, timedelta
# import time
# import os

# # í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
# keywords = ["ì´ì¤€ì„, ì´ì¬ëª…, ê¹€ë¬¸ìˆ˜, ëŒ€ì„ "]
# start_date_str = "2025.05.19"
# end_date_str = "2025.05.21"
# max_articles_per_keyword = 6000  # ìˆ˜ì§‘ ì œí•œ

# headers = {
#     "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
# }

# # ì…€ë ˆë‹ˆì›€ ì„¤ì •
# options = Options()
# options.add_argument("--headless")
# options.add_argument("--disable-gpu")
# options.add_argument("window-size=1920x1080")
# driver = webdriver.Chrome(options=options)

# def str_to_date(s):
#     return datetime.strptime(s, "%Y.%m.%d")

# # ê¸°ì‚¬ ë³¸ë¬¸ íŒŒì‹± í•¨ìˆ˜ (ë‚´ìš© ì œì™¸)
# def fetch_article_content(url):
#     def parse_soup(soup):
#         title = (
#             soup.select_one(".media_end_head_headline") or
#             soup.find("h2") or
#             soup.find("div", class_="news_title") or
#             soup.find("div", class_="end_tit")
#         )
#         date = (
#             soup.select_one("span.media_end_head_info_datestamp_time._ARTICLE_DATE_TIME") or
#             soup.find("em", class_="date") or
#             soup.find("span", class_="date") or
#             soup.find("div", class_="date")
#         )
#         if title and date:
#             return title.text.strip(), date.text.strip()
#         return None, None

#     try:
#         res = requests.get(url, headers=headers, timeout=5)
#         soup = BeautifulSoup(res.text, "html.parser")
#         result = parse_soup(soup)
#         if all(result):
#             return result
#     except:
#         pass

#     try:
#         driver.get(url)
#         time.sleep(2)
#         soup = BeautifulSoup(driver.page_source, "html.parser")
#         return parse_soup(soup)
#     except:
#         return None, None

# # âœ… ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ í•¨ìˆ˜
# def collect_news_links(search_url):
#     try:
#         res = requests.get(search_url, headers=headers, timeout=5)
#         soup = BeautifulSoup(res.text, "html.parser")
#         links = list({a['href'] for a in soup.select(
#             "a[href^='https://n.news.naver.com'], a[href^='https://m.entertain.naver.com']") if a.get('href')})
#         if len(links) >= 5:
#             return links
#     except:
#         pass
#     try:
#         driver.get(search_url)
#         time.sleep(2)
#         soup = BeautifulSoup(driver.page_source, "html.parser")
#         links = list({a['href'] for a in soup.select(
#             "a[href^='https://n.news.naver.com'], a[href^='https://m.entertain.naver.com']") if a.get('href')})
#         return links
#     except:
#         return []

# # âœ… í‚¤ì›Œë“œë³„ ì „ì²´ í¬ë¡¤ë§ ë£¨í”„
# for keyword in keywords:
#     print(f"\nğŸ” í‚¤ì›Œë“œ [{keyword}] í¬ë¡¤ë§ ì‹œì‘")
#     file_name = f"all_news_{keyword}_20250519~20250521.xlsx"

#     if os.path.exists(file_name):
#         wb = load_workbook(file_name)
#         ws = wb.active
#     else:
#         wb = Workbook()
#         ws = wb.active
#         ws.title = "ë‰´ìŠ¤ê¸°ì‚¬"
#         ws.append(['ì œëª©', 'ë‚ ì§œ', 'URL'])

#     ws.column_dimensions['A'].width = 60
#     ws.column_dimensions['B'].width = 30
#     ws.column_dimensions['C'].width = 60

#     visited_links = set()
#     collected = 0
#     current_date = str_to_date(start_date_str)
#     end_date = str_to_date(end_date_str)

#     while current_date <= end_date and collected < max_articles_per_keyword:
#         ds = current_date.strftime("%Y.%m.%d")
#         de = ds
#         print(f"----- ğŸ“… {ds} í¬ë¡¤ë§ ì¤‘ -----")
#         fail_count = 0
#         max_fails = 5

#         for start in range(1, 2000, 10):
#             if collected >= max_articles_per_keyword:
#                 break

#             search_url = (
#                 f"https://search.naver.com/search.naver?where=news&query={keyword}"
#                 f"&sm=tab_opt&sort=2&photo=0&field=0&pd=3&ds={ds}&de={de}&start={start}"
#             )
#             news_links = collect_news_links(search_url)

#             if not news_links:
#                 fail_count += 1
#                 print(f"âš ï¸ ë‰´ìŠ¤ ì—†ìŒ (ì‹¤íŒ¨ {fail_count}/{max_fails})")
#                 if fail_count >= max_fails:
#                     print("â›” ì—°ì† ì‹¤íŒ¨ íšŸìˆ˜ ì´ˆê³¼, ì¢…ë£Œ")
#                     break
#                 continue
#             else:
#                 fail_count = 0

#             for href in news_links:
#                 if href in visited_links or collected >= max_articles_per_keyword:
#                     continue
#                 visited_links.add(href)

#                 title, date = fetch_article_content(href)
#                 if title and date:
#                     collected += 1
#                     print(f"âœ¨ ({collected}) {date} - {title}")
#                     ws.append([title, date, href])
#                 else:
#                     print("âš ï¸ ë³¸ë¬¸ ëˆ„ë½:", href)

#             time.sleep(1)

#         current_date += timedelta(days=1)

#     wb.save(file_name)
#     print(f"\nâœ… ì €ì¥ ì™„ë£Œ: {file_name}")

# driver.quit()

# import pandas as pd
# import numpy as np
# import requests
# import json
# import html
# import time

# # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# # 1) ëŒ“ê¸€ í¬ë¡¤ë§ í•¨ìˆ˜ (ì‘ì„±ì ID, ì‘ì„± ì‹œê°„, ë‚´ìš©ê¹Œì§€ ìˆ˜ì§‘)
# # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# def crawl_comments(url, template="default_society", pool="cbox5", page_size=20):
#     oid = url.split("article/")[1].split("/")[0]
#     aid = url.split("article/")[1].split("/")[1].split("?")[0]
#     page = 1
#     rows = []
#     headers = {"User-Agent": "Mozilla/5.0", "Referer": url}

#     while True:
#         callback = f"jQuery{int(time.time()*1000)}"
#         api_url = (
#             "https://apis.naver.com/commentBox/cbox/web_neo_list_jsonp.json"
#             f"?ticket=news&templateId={template}&pool={pool}"
#             f"&_callback={callback}&lang=ko&country=KR"
#             f"&objectId=news{oid}%2C{aid}"
#             f"&pageSize={page_size}&indexSize=10&listType=OBJECT&pageType=more"
#             f"&page={page}&sort=FAVORITE"
#         )
#         res = requests.get(api_url, headers=headers)
#         text = res.text
#         json_str = text[text.find('(')+1 : text.rfind(')')]
#         data = json.loads(json_str)

#         comment_list = data.get("result", {}).get("commentList", [])
#         if not comment_list:
#             break

#         for c in comment_list:
#             rows.append({
#                 "url": url,
#                 "comment_author_id": c.get("maskedUserId"),
#                 "comment_published": c.get("modTime", c.get("createTime")),
#                 "comment_text": html.unescape(c.get("contents", ""))
#             })

#         total = data.get("result", {}).get("totalCommentCount", 0)
#         if page * page_size >= total:
#             break

#         page += 1
#         time.sleep(0.3)

#     return pd.DataFrame(rows)


# # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# # 2) ìµœì í™”ëœ ë´‡ í•„í„°ë§ í•¨ìˆ˜ (ì‘ì„±ìë³„ ê·¸ë£¹í•‘, ë¯¼ê°ë„ â†‘)
# # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# def optimized_filter_bot_comments(df):
#     print("=== ìµœì í™”ëœ ë´‡ ëŒ“ê¸€ í•„í„°ë§ ì‹œì‘ (ë†’ì€ ë¯¼ê°ë„) ===")
#     df = df.copy()

#     # 1) timestamp, ê¸¸ì´, ë‹¨ì–´ ë°˜ë³µìœ¨ ì „ì²˜ë¦¬
#     df['ts'] = pd.to_datetime(df['comment_published'])
#     df.sort_values(['comment_author_id','ts'], inplace=True)
#     df['length'] = df['comment_text'].str.len()
#     df['repetition_ratio'] = (
#         df['comment_text']
#           .str.split()
#           .map(lambda w: 1 - len(set(w)) / len(w) if len(w)>0 else 0)
#     )

#     # 2) ì‹œê°„ ê°„ê²©(diff) ê³„ì‚°
#     df['interval'] = df.groupby('comment_author_id')['ts'] \
#                        .diff() \
#                        .dt.total_seconds()

#     # 3) ì‚¬ìš©ìë³„ í†µê³„ëŸ‰ ì§‘ê³„
#     stats = df.groupby('comment_author_id').agg(
#         comment_count=('comment_text','size'),
#         time_var_sec=('interval', lambda x: np.var(x.dropna()) if len(x.dropna())>=2 else np.nan),
#         length_var=('length', 'var'),
#         avg_rep_ratio=('repetition_ratio', 'mean')
#     )
#     stats['time_var_hr'] = stats['time_var_sec'] / 3600

#     # 4) AI ìƒì„± ì˜ì‹¬ í™•ë¥  ë²¡í„°í™” (ê¸°ì¡´ íœ´ë¦¬ìŠ¤í‹±)
#     def ai_prob_series(texts: pd.Series) -> pd.Series:
#         s = texts.fillna('').astype(str)
#         sent_lens = s.str.split('.').map(lambda ss: [len(x) for x in ss if x])
#         var_len = sent_lens.map(lambda L: np.var(L) if len(L)>1 else np.nan)
#         punct_cnt = s.str.count(r'[.!?]')
#         sent_cnt  = s.str.count(r'\.') + 1
#         punct_ratio = punct_cnt / sent_cnt
#         emo_ind = s.str.contains('ã…‹ã…‹|ã…ã…|ã… ã… |!!!|\?\?\?')
#         return (
#             (var_len < 50).fillna(0).astype(float) +
#             (punct_ratio > 0.8).astype(float) +
#             (~emo_ind & (s.str.len()>50)).astype(float)
#         ) / 3.0

#     df['ai_prob'] = ai_prob_series(df['comment_text'])
#     ai_stats = df.groupby('comment_author_id')['ai_prob'].mean().rename('avg_ai_prob')

#     # 5) í†µí•© & ë´‡ íŒì • (ë¯¼ê°ë„ â†‘)
#     stats = stats.join(ai_stats, how='left').fillna(0)

#     # 1) ë¹ ë¥¸ ì†ë„: ëŒ“ê¸€ â‰¥5ê°œ & ì‹œê°„ ë¶„ì‚°(hr) <0.2
#     stats['cond_speed'] = (
#         (stats['comment_count'] >= 5) &
#         (stats['time_var_hr'] < 0.2)
#     ).astype(int)

#     # 2) AI ìƒì„± ì˜ì‹¬: avg_ai_prob > 0.5
#     stats['cond_ai'] = (stats['avg_ai_prob'] > 0.5).astype(int)

#     # 3) íœ´ë¦¬ìŠ¤í‹± ì˜ì‹¬: suspicious_score > 0.3
#     stats['suspicious_score'] = (
#         (stats['length_var'] < 200).astype(float) * 0.2 +
#         (stats['avg_rep_ratio'] > 0.6).astype(float) * 0.3
#     )
#     stats['cond_suspicious'] = (stats['suspicious_score'] > 0.3).astype(int)

#     # 4) ë‹¨ì–´ ë°˜ë³µìœ¨ ê³¼ë‹¤: avg_rep_ratio > 0.6
#     stats['high_rep'] = (stats['avg_rep_ratio'] > 0.6).astype(int)

#     # 5) ëŒ“ê¸€ ê¸¸ì´ ë‹¨ì¡°: length_var < 200
#     stats['low_len_var'] = (stats['length_var'] < 200).astype(int)

#     # **ì´ 5ê°œ ì§€í‘œ ì¤‘ â‰¥2ê°œ** ë§Œì¡± ì‹œ ë´‡
#     stats['is_bot'] = (
#         stats['cond_speed'] +
#         stats['cond_ai'] +
#         stats['cond_suspicious'] +
#         stats['high_rep'] +
#         stats['low_len_var']
#     ) >= 2

#     bot_users = stats.index[stats['is_bot']].tolist()
#     filtered = df[~df['comment_author_id'].isin(bot_users)].drop(
#         ['ts','length','repetition_ratio','interval','ai_prob'], axis=1
#     )

#     print(f"ì›ë³¸: {len(df):,}ê°œ â†’ í•„í„°ë§ í›„: {len(filtered):,}ê°œ")
#     return filtered, stats.reset_index()


# # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# # 3) ë©”ì¸ ì‹¤í–‰
# # â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# if __name__ == "__main__":
#     # (1) ì›ë³¸ ì—‘ì…€ì—ì„œ URL ë¶ˆëŸ¬ì˜¤ê¸°
#     excel_path = "all_news_ì•ˆì² ìˆ˜_20250522~20250524.xlsx"
#     df_urls    = pd.read_excel(excel_path)
#     urls       = df_urls['Naverlink'] if 'Naverlink' in df_urls else df_urls['URL']

#     # (2) ì „ì²´ ëŒ“ê¸€ í¬ë¡¤ë§
#     all_comments = []
#     for idx, url in enumerate(urls.dropna(), 1):
#         print(f"[{idx}/{len(urls)}] ëŒ“ê¸€ ìˆ˜ì§‘: {url}")
#         try:
#             df_c = crawl_comments(url)
#             all_comments.append(df_c)
#         except Exception as e:
#             print("  ì˜¤ë¥˜:", e)
#             continue

#     comments_df = pd.concat(all_comments, ignore_index=True)

#     # (3) ë´‡ í•„í„°ë§
#     filtered_df, stats_df = optimized_filter_bot_comments(comments_df)

#     # (4) ê²°ê³¼ ì €ì¥
#     filtered_df.to_csv('filtered_comments_ì•ˆì² ìˆ˜.csv', index=False, encoding='utf-8-sig')
#     stats_df.to_csv('bot_analysis_stats_ì•ˆì² ìˆ˜.csv', index=False, encoding='utf-8-sig')

#     print("âœ… ì™„ë£Œ! filtered_comments.csv ì™€ bot_analysis_stats.csv ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")

from googleapiclient.discovery import build
from googleapiclient.errors import HttpError
import pandas as pd
import time
from datetime import datetime, timedelta
from collections import deque
from concurrent.futures import ThreadPoolExecutor
import threading

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ì„¤ì •
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
API_KEYS = [
    "AIzaSyBn1v3HjJWhqh_YHyF6sJkV41cOyLabemI", # 1
    "AIzaSyAg6WNmdx7MYigHbrA1TMf6cO89tuuqSX8", # 2
    "AIzaSyBujNpcCzdVU4QBstIIXNR2yc85WxYWdHQ", # 3
    "AIzaSyClz3cRUGIoe7ql0-KAP2b-tTIcONJkOzo", # 4
    "AIzaSyD80A71_82-_WTDVYruqPBTG0fr034ajBk", # 5
    "AIzaSyDFhQRYh_cZ6lAgjRgFQYl9BzaTE58Qh1Q", # 6
    "AIzaSyCmZxMmIIyBNfw6tR_5-xQUdlPMlVHU-_8", # 7
    "AIzaSyBh265b37BHH_0j62feHIcqXH7QYDaV-Ag", # 8
    "AIzaSyCWPFZYM197mcRNCFac5klW-WPhLjbQk74", # 9
    "AIzaSyDMRehGlKMt-Dy2czwz6DbAco18hL_qdR8", # 10
]

CHANNEL_IDS = [
    "UCugbqfMO94F9guLEb6Olb2A",  # í•œê²¨ë ˆ
    "UCF4Wxdo3inmxP-Y59wXDsFw",  # MBC
    "UCHXvjavEtkPFJCfGlm0wTXw",  # ê²½í–¥
    "UCnHyx6H7fhKfUoAAaVGYvxQ",  # ë™ì•„
    "UCWlV3Lz_55UaX4JsMj-z__Q",  # ì¡°ì„ 
    "UCH3mJ-nHxjjny2FJbJaqiDA"   # ì¤‘ì•™
]

KEYWORDS = ["ëŒ€ì„ ", "ê¹€ë¬¸ìˆ˜", "ì´ì¬ëª…", "ì´ì¤€ì„"]
START_DATE = "2025-05-19"
END_DATE   = "2025-05-21"
TOTAL_LIMIT = 100_000
PER_CHANNEL_LIMIT = TOTAL_LIMIT // len(CHANNEL_IDS)
POLITICS_CATEGORY = "25"  # News & Politics

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ì „ì—­ ë³€ìˆ˜ ì´ˆê¸°í™”
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
_exhausted = set()
_services = {}
results = []
counts = {cid: 0 for cid in CHANNEL_IDS}
video_queues = {cid: deque() for cid in CHANNEL_IDS}

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# í—¬í¼ í•¨ìˆ˜
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
def get_svc(key):
    if key not in _services:
        _services[key] = build("youtube", "v3", developerKey=key)
    return _services[key]

def rotate_call(fn, *args, **kwargs):
    for k in API_KEYS:
        if k in _exhausted: continue
        try:
            return fn(get_svc(k), *args, **kwargs)
        except HttpError as e:
            if e.resp.status == 403 and "quota" in str(e).lower():
                print(f"âš ï¸ {k} quotaExceeded â†’ ì œì™¸")
                _exhausted.add(k)
                continue
            else:
                raise
    raise RuntimeError("ëª¨ë“  í‚¤ ì†Œì§„ë¨")

def search_videos(svc, channelId, q, start, end, pageToken=None, category=None):
    params = {
        "part": "id",
        "channelId": channelId,
        "type": "video",
        "publishedAfter": start + "Z",
        "publishedBefore": end + "Z",
        "maxResults": 50,
        "pageToken": pageToken,
        "fields": "nextPageToken,items(id/videoId)"
    }
    if q:
        params["q"] = q
    if category:
        params["videoCategoryId"] = category
    return svc.search().list(**params).execute()

def fetch_video_info(svc, videoId):
    return svc.videos().list(
        part="snippet,statistics",
        id=videoId,
        fields="items(snippet(title,channelTitle,publishedAt,categoryId,tags),"
               "statistics(viewCount,likeCount,commentCount))"
    ).execute()

def fetch_comments_with_replies(svc, videoId, pageToken=None):
    """
    top-level ëŒ“ê¸€ê³¼, 
   ê·¸ ëŒ“ê¸€ ì‘ì„±ìë¥¼ ë©˜ì…˜(@ì‘ì„±ìID)í•œ ë‹µê¸€ë§Œ í•¨ê»˜ ê°€ì ¸ì˜µë‹ˆë‹¤.
    """
    resp = svc.commentThreads().list(
        part="snippet,replies",
        videoId=videoId,
        maxResults=100,
        pageToken=pageToken,
        textFormat="plainText",
        fields="nextPageToken,items(snippet/topLevelComment/id,snippet/topLevelComment/snippet(authorChannelId/value,textDisplay,publishedAt,likeCount),replies/comments(id,snippet(authorChannelId/value,textDisplay,publishedAt)))"
    ).execute()

    out = []
    for thread in resp.get("items", []):
        top = thread["snippet"]["topLevelComment"]["snippet"]
        top_id     = thread["snippet"]["topLevelComment"]["id"]
        top_auth   = top.get("authorChannelId", {}).get("value", "")
        top_text   = top.get("textDisplay", "")
        top_publ   = top.get("publishedAt", "")
        top_likes  = top.get("likeCount", 0)

        # 1) ì›ëŒ“ê¸€ ì €ì¥
        out.append({
            "comment_id":        top_id,
            "comment_author_id": top_auth,
            "comment_text":      top_text,
            "comment_published": top_publ,
            "comment_likes":     top_likes,
            "is_reply":          False,
            "reply_to_id":       None,
            "reply_author_id":   None,
            "reply_text":        None,
            "reply_published":   None
        })

        # 2) ë‹µê¸€ ì¤‘ â€˜@ì›ëŒ“ê¸€ì‘ì„±ìIDâ€™ ë©˜ì…˜ë§Œ ì €ì¥
        for reply in thread.get("replies", {}).get("comments", []):
            r = reply["snippet"]
            if f"@{top_auth}" in r.get("textDisplay", ""):
                out.append({
                    "comment_id":        top_id,
                    "comment_author_id": top_auth,
                    "comment_text":      top_text,
                    "comment_published": top_publ,
                    "comment_likes":     top_likes,
                    "is_reply":          True,
                    "reply_to_id":       reply["id"],
                    "reply_author_id":   r.get("authorChannelId", {}).get("value", ""),
                    "reply_text":        r.get("textDisplay", ""),
                    "reply_published":   r.get("publishedAt", "")
                })
    return resp.get("nextPageToken"), out

def fetch_channel_uploads(svc, channelId, pageToken=None):
    # ì±„ë„ ì—…ë¡œë“œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ ID ì¡°íšŒ
    ch_resp = svc.channels().list(
        part="contentDetails",
        id=channelId,
        fields="items/contentDetails/relatedPlaylists/uploads"
    ).execute()
    items = ch_resp.get("items")
    if not items:
        # ì±„ë„ ì •ë³´ê°€ ì—†ìœ¼ë©´ ë¹ˆ ê²°ê³¼ ë°˜í™˜
        return {"items": [], "nextPageToken": None}

    upload_pl = items[0]["contentDetails"]["relatedPlaylists"]["uploads"]
    # ì—…ë¡œë“œ í”Œë ˆì´ë¦¬ìŠ¤íŠ¸ì—ì„œ ì˜ìƒ ID ì¡°íšŒ
    pl_resp = svc.playlistItems().list(
        part="contentDetails",
        playlistId=upload_pl,
        maxResults=50,
        pageToken=pageToken,
        fields="nextPageToken,items/contentDetails/videoId"
    ).execute()
    return pl_resp

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Phase 1: ì˜ìƒ ID í ìƒì„±
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
start = datetime.fromisoformat(START_DATE)
end   = datetime.fromisoformat(END_DATE)
for d in range((end - start).days):
    day1 = (start + timedelta(days=d)).isoformat()
    day2 = (start + timedelta(days=d+1)).isoformat()

    for cid in CHANNEL_IDS:
        # í‚¤ì›Œë“œë³„
        for kw in KEYWORDS:
            token = None
            while True:
                resp = rotate_call(search_videos, cid, kw, day1, day2, token)
                for it in resp.get("items", []):
                    video_queues[cid].append(it["id"]["videoId"])
                token = resp.get("nextPageToken")
                if not token:
                    break
        # ì¹´í…Œê³ ë¦¬ë³„
        token = None
        while True:
            resp = rotate_call(search_videos, cid, None, day1, day2, token, category=POLITICS_CATEGORY)
            for it in resp.get("items", []):
                video_queues[cid].append(it["id"]["videoId"])
            token = resp.get("nextPageToken")
            if not token:
                break
    
        if not video_queues[cid]:
            token2 = None
            while True:
                resp2 = rotate_call(fetch_channel_uploads, cid, token2)
                for it2 in resp2.get("items", []):
                    video_queues[cid].append(it2["contentDetails"]["videoId"])
                token2 = resp2.get("nextPageToken")
                if not token2:
                    break

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# Phase 2: ë¼ìš´ë“œë¡œë¹ˆ ëŒ“ê¸€ ìˆ˜ì§‘
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
done_all = False
while sum(counts.values()) < TOTAL_LIMIT and not done_all:
    done_all = True
    for cid in CHANNEL_IDS:
        if counts[cid] >= PER_CHANNEL_LIMIT or not video_queues[cid]:
            continue
        done_all = False
        vid = video_queues[cid].popleft()

        # ì˜ìƒ ì •ë³´
        info = rotate_call(fetch_video_info, vid)["items"][0]
        # íƒœê·¸ ì •ë³´ ìˆ˜ì§‘ X ; ëŒ“ê¸€ ìˆ˜ì§‘ ì ê²Œ ë˜ëŠ” ì±„ë„
        '''
        tags = info["snippet"].get("tags", [])
        if not any(kw in tag for tag in KEYWORDS for tag in tags):
            continue
        '''
        base = {
            "video_id": vid,
            "channel": info["snippet"]["channelTitle"],
            "video_title": info["snippet"]["title"],
            "video_published": info["snippet"]["publishedAt"],
            "video_views": info["statistics"].get("viewCount", 0),
            "video_likes": info["statistics"].get("likeCount", 0),
            "video_comment_count": info["statistics"].get("commentCount", 0),
            "video_category_id": info["snippet"]["categoryId"]
        }

        # ëŒ“ê¸€+ë©˜ì…˜ ë‹µê¸€ ìˆ˜ì§‘
        token = None
        while counts[cid] < PER_CHANNEL_LIMIT:
            try:
                token, items = rotate_call(fetch_comments_with_replies, vid, token)
            except HttpError as e:
                # commentsDisabledì¸ ê²½ìš° ê±´ë„ˆë›°ê¸°
                if e.resp.status == 403 and e.error_details and any(d.get('reason')=='commentsDisabled' for d in e.error_details):
                    print(f"âš ï¸ {vid}: commentsDisabled â†’ ìŠ¤í‚µ")
                    break
                else:
                    raise
            for c in items:
                results.append({**base, **c})
                counts[cid] += 1
                if counts[cid] >= PER_CHANNEL_LIMIT or sum(counts.values()) >= TOTAL_LIMIT:
                    break
            if not token or counts[cid] >= PER_CHANNEL_LIMIT:
                break

        time.sleep(0.1)

    # end for CHANNEL_IDS
# end while

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ê²°ê³¼ ì €ì¥
# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
pd.DataFrame(results).to_csv("all_news_ê¹€ë¬¸ìˆ˜, ì´ì¬ëª…, ì´ì¤€ì„, ëŒ€ì„ _20250519~20250521.csv", index=False, encoding="utf-8-sig")
print("ì±„ë„ë³„ counts:", counts)
print("ì´ ëŒ“ê¸€:", len(results))